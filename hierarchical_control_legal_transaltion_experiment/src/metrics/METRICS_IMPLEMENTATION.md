# ç°ä»£æœºå™¨ç¿»è¯‘è¯„ä¼°æŒ‡æ ‡å®ç°æ€»ç»“

## ğŸ“‹ å·²å®ç°çš„æŒ‡æ ‡

åŸºäºæœ€æ–°çš„æœºå™¨ç¿»è¯‘è¯„ä¼°ç ”ç©¶ï¼ˆ2023-2024ï¼‰ï¼Œæˆ‘ä»¬å®ç°äº†ä»¥ä¸‹5ç±»è¯„ä¼°æŒ‡æ ‡ï¼š

### 1. BLEU (Bilingual Evaluation Understudy)
- **æ–‡ä»¶**: `src/metrics/bleu.py`
- **ä¾èµ–**: `sacrebleu>=2.3.1`
- **ç‰¹ç‚¹**: ä¼ ç»Ÿn-gramç²¾ç¡®åŒ¹é…ï¼Œå¿«é€Ÿè®¡ç®—
- **ç”¨é€”**: åŸºçº¿å¯¹æ¯”ã€å¿«é€Ÿè¯„ä¼°

### 2. chrF++ (Character n-gram F-score)
- **æ–‡ä»¶**: `src/metrics/chrf.py`
- **ä¾èµ–**: `sacrebleu>=2.3.1`
- **ç‰¹ç‚¹**: å­—ç¬¦çº§è¯„ä¼°ï¼Œç‰¹åˆ«é€‚åˆä¸­æ–‡
- **ç”¨é€”**: å¤šè¯­è¨€è¯„ä¼°ã€ä¸éœ€è¦åˆ†è¯

### 3. BERTScore
- **æ–‡ä»¶**: `src/metrics/bertscore.py`
- **ä¾èµ–**: `bert-score>=0.3.13`
- **ç‰¹ç‚¹**: åŸºäºBERTçš„è¯­ä¹‰ç›¸ä¼¼åº¦
- **ç”¨é€”**: è¯­ä¹‰è´¨é‡è¯„ä¼°

### 4. COMET (Crosslingual Optimized Metric for Evaluation of Translation)
- **æ–‡ä»¶**: `src/metrics/comet.py`
- **ä¾èµ–**: `unbabel-comet>=2.2.0`
- **ç‰¹ç‚¹**: WMT2022æœ€ä½³æŒ‡æ ‡ï¼Œé«˜åº¦ç›¸å…³äººç±»åˆ¤æ–­
- **ç”¨é€”**: é«˜è´¨é‡è¯„ä¼°

### 5. GEMBA (GPT Estimation Metric Based Assessment)
- **æ–‡ä»¶**: `src/metrics/gemba_mqm.py`
- **ä¾èµ–**: OpenAI API (GPT-4)
- **å®˜æ–¹å®ç°**: https://github.com/MicrosoftTranslator/GEMBA
- **ä¸¤ç§æ–¹æ³•**:
  - **GEMBA-MQM**: è¯¦ç»†é”™è¯¯æ£€æµ‹å’ŒMQMè¯„åˆ†
  - **GEMBA-DA**: ç›´æ¥è´¨é‡è¯„ä¼°ï¼ˆæ¨èï¼‰
- **ç‰¹ç‚¹**: WMT2023æœ€ä½³æŒ‡æ ‡ï¼Œæœ€æ¥è¿‘äººå·¥è¯„ä¼°
- **ç”¨é€”**: æœ€ç»ˆè¯„ä¼°ã€é”™è¯¯è¯Šæ–­

## ğŸ—ï¸ é¡¹ç›®ç»“æ„

```
src/metrics/
â”œâ”€â”€ __init__.py           # æ¨¡å—å¯¼å‡º
â”œâ”€â”€ bleu.py              # BLEUæŒ‡æ ‡
â”œâ”€â”€ chrf.py              # chrF++æŒ‡æ ‡
â”œâ”€â”€ bertscore.py         # BERTScoreæŒ‡æ ‡
â”œâ”€â”€ comet.py             # COMETæŒ‡æ ‡
â”œâ”€â”€ gemba_mqm.py         # GEMBA-MQMå’ŒGEMBA-DAæŒ‡æ ‡
â”œâ”€â”€ metric_suite.py      # ç»Ÿä¸€çš„æŒ‡æ ‡å¥—ä»¶
â””â”€â”€ README.md            # è¯¦ç»†æ–‡æ¡£

test_metrics.py          # æµ‹è¯•è„šæœ¬
requirements.txt         # ä¾èµ–å£°æ˜ï¼ˆå·²æ›´æ–°ï¼‰
```

## ğŸš€ å¿«é€Ÿä½¿ç”¨

### åŸºç¡€ä½¿ç”¨

```python
from src.metrics import MetricSuite

# åˆ›å»ºæŒ‡æ ‡å¥—ä»¶ï¼ˆå¿«é€ŸæŒ‡æ ‡ï¼‰
suite = MetricSuite(metrics=['bleu', 'chrf'])

# è®¡ç®—åˆ†æ•°
scores = suite.compute(
    source="åˆåŒåŒæ–¹åº”å½“éµå®ˆæœ¬åè®®çš„æ‰€æœ‰æ¡æ¬¾ã€‚",
    prediction="The parties shall comply with all terms of this agreement.",
    reference="Contracting parties must comply with all provisions of this agreement."
)

print(scores)
# {'bleu': 45.23, 'chrf': 67.89}
```

### GEMBAä½¿ç”¨ï¼ˆæ¨èï¼‰

```python
from src.metrics import GEMBAMetric

# GEMBA-DAï¼ˆæ›´å¿«ï¼Œæ¨èç”¨äºæ‰¹é‡è¯„ä¼°ï¼‰
gemba_da = GEMBAMetric(method="GEMBA-DA", model="gpt-4")
score = gemba_da.sentence_score(
    source="åˆåŒåŒæ–¹åº”å½“éµå®ˆæœ¬åè®®çš„æ‰€æœ‰æ¡æ¬¾ã€‚",
    prediction="The parties shall comply with all terms.",
    source_lang="Chinese",
    target_lang="English"
)
print(f"GEMBA-DA: {score:.2f}/100")

# GEMBA-MQMï¼ˆè¯¦ç»†é”™è¯¯åˆ†æï¼‰
gemba_mqm = GEMBAMetric(method="GEMBA-MQM", model="gpt-4")
result = gemba_mqm.compute(
    sources=["åˆåŒåŒæ–¹åº”å½“éµå®ˆæœ¬åè®®çš„æ‰€æœ‰æ¡æ¬¾ã€‚"],
    predictions=["The parties shall comply with all terms."],
    source_lang="Chinese",
    target_lang="English"
)
print(f"åˆ†æ•°: {result['mean']:.2f}")
print(f"é”™è¯¯: {result['results'][0]['errors']}")
```

## ğŸ“Š æŒ‡æ ‡å¯¹æ¯”

| æŒ‡æ ‡ | é€Ÿåº¦ | èµ„æº | äººç±»ç›¸å…³æ€§ | WMTæ’å | æ¨èåœºæ™¯ |
|------|------|------|-----------|---------|----------|
| BLEU | âš¡âš¡âš¡ | æ—  | â­â­ | åŸºçº¿ | å¿«é€Ÿå¼€å‘ |
| chrF++ | âš¡âš¡âš¡ | æ—  | â­â­â­ | è‰¯å¥½ | å¤šè¯­è¨€ |
| BERTScore | âš¡âš¡ | 1.4GB | â­â­â­â­ | ä¼˜ç§€ | è¯­ä¹‰è¯„ä¼° |
| COMET | âš¡âš¡ | 2.3GB | â­â­â­â­â­ | WMT2022#1 | é«˜è´¨é‡è¯„ä¼° |
| GEMBA-DA | âš¡ | GPT-4 | â­â­â­â­â­ | WMT2023#1 | æœ€ç»ˆè¯„ä¼° |
| GEMBA-MQM | âš¡ | GPT-4 | â­â­â­â­â­ | WMT2023 | é”™è¯¯è¯Šæ–­ |

## ğŸ’¡ æ¨èé…ç½®

### å¼€å‘é˜¶æ®µï¼ˆå¿«é€Ÿè¿­ä»£ï¼‰
```python
suite = MetricSuite(metrics=['bleu', 'chrf'])
```

### éªŒè¯é˜¶æ®µï¼ˆå¹³è¡¡è´¨é‡ï¼‰
```python
suite = MetricSuite(metrics=['bleu', 'chrf', 'comet'])
```

### æœ€ç»ˆè¯„ä¼°ï¼ˆWMTæ ‡å‡†ï¼‰
```python
suite = MetricSuite(
    metrics=['comet', 'gemba'],
    gemba_method='GEMBA-DA'
)
```

### é”™è¯¯åˆ†æï¼ˆè¯¦ç»†è¯Šæ–­ï¼‰
```python
gemba_mqm = GEMBAMetric(method="GEMBA-MQM", model="gpt-4")
```

## ğŸ”§ å®‰è£…ä¾èµ–

```bash
# å®‰è£…æ‰€æœ‰ä¾èµ–
pip install -r requirements.txt

# æˆ–åˆ†åˆ«å®‰è£…
pip install sacrebleu              # BLEU, chrF++
pip install bert-score             # BERTScore
pip install unbabel-comet          # COMET
# GEMBAä½¿ç”¨ç°æœ‰çš„OpenAI APIé…ç½®
```

## ğŸ“ åœ¨å®éªŒä¸­é›†æˆ

è¦åœ¨ç°æœ‰çš„ `metrics.py` ä¸­é›†æˆè¿™äº›æŒ‡æ ‡ï¼š

```python
from src.metrics import MetricSuite

class LegalTranslationMetrics:
    def __init__(self):
        # åŸæœ‰çš„æ³•å¾‹ä¸“ç”¨æŒ‡æ ‡
        self.deontic_mapping = {...}
        
        # æ·»åŠ ç°ä»£MTæŒ‡æ ‡
        self.mt_metrics = MetricSuite(
            metrics=['bleu', 'chrf', 'comet'],
            lang='zh'
        )
    
    def calculate_all_metrics(self, source, target, reference, 
                             src_lang, tgt_lang, term_table):
        # æ³•å¾‹ä¸“ç”¨æŒ‡æ ‡
        legal_metrics = {
            'termbase_accuracy': self.calculate_termbase_accuracy(...),
            'deontic_preservation': self.calculate_deontic_preservation(...),
            'conditional_logic': self.calculate_conditional_logic_preservation(...)
        }
        
        # ç°ä»£MTæŒ‡æ ‡
        mt_metrics = self.mt_metrics.compute(source, target, reference)
        
        # åˆå¹¶è¿”å›
        return {**legal_metrics, **mt_metrics}
```

## ğŸ§ª æµ‹è¯•

è¿è¡Œæµ‹è¯•è„šæœ¬ï¼š

```bash
python test_metrics.py
```

æµ‹è¯•åŒ…æ‹¬ï¼š
1. âœ… åŸºç¡€æŒ‡æ ‡ï¼ˆBLEU, chrF++ï¼‰
2. âœ… æŒ‡æ ‡å¥—ä»¶
3. âš ï¸  é«˜çº§æŒ‡æ ‡ï¼ˆBERTScore, COMETï¼‰- éœ€è¦ä¸‹è½½æ¨¡å‹
4. âš ï¸  GEMBAæŒ‡æ ‡ - éœ€è¦OpenAI API

## ğŸ“š å‚è€ƒæ–‡çŒ®

### è®ºæ–‡å¼•ç”¨

1. **GEMBA-DA** (WMT2023æœ€ä½³)
   ```
   Kocmi, T., & Federmann, C. (2023).
   Large Language Models Are State-of-the-Art Evaluators of Translation Quality.
   EAMT 2023.
   ```

2. **GEMBA-MQM** (è¯¦ç»†é”™è¯¯åˆ†æ)
   ```
   Kocmi, T., & Federmann, C. (2023).
   GEMBA-MQM: Detecting Translation Quality Error Spans with GPT-4.
   WMT 2023.
   ```

3. **COMET** (WMT2022æœ€ä½³)
   ```
   Rei, R., et al. (2020).
   COMET: A Neural Framework for MT Evaluation.
   EMNLP 2020.
   ```

### å®˜æ–¹é“¾æ¥

- **GEMBA**: https://github.com/MicrosoftTranslator/GEMBA
- **COMET**: https://github.com/Unbabel/COMET
- **BERTScore**: https://github.com/Tiiiger/bert_score
- **SacreBLEU**: https://github.com/mjpost/sacrebleu

## âš ï¸ é‡è¦è¯´æ˜

1. **æ¨¡å‹ä¸‹è½½**: BERTScoreå’ŒCOMETé¦–æ¬¡ä½¿ç”¨ä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼ˆè¾ƒå¤§ï¼‰
2. **GPUæ¨è**: BERTScoreå’ŒCOMETæ”¯æŒGPUï¼Œæ¨èä½¿ç”¨ä»¥æé«˜é€Ÿåº¦
3. **APIæˆæœ¬**: GEMBAä½¿ç”¨GPT-4 APIï¼Œä¼šäº§ç”Ÿè´¹ç”¨
   - GEMBA-DA: ~500-800 tokens/æ¡
   - GEMBA-MQM: ~800-1200 tokens/æ¡
4. **è¯„ä¼°æ—¶é—´**: GEMBAè¾ƒæ…¢ï¼Œå»ºè®®ç”¨äºæœ€ç»ˆè¯„ä¼°æˆ–æŠ½æ ·åˆ†æ

## ğŸ¯ æœ€ä½³å®è·µ

1. **å¼€å‘æ—¶**: ä½¿ç”¨BLEU/chrFå¿«é€Ÿè¿­ä»£
2. **éªŒè¯æ—¶**: æ·»åŠ COMETç¡®ä¿è´¨é‡
3. **å‘å¸ƒå‰**: ä½¿ç”¨GEMBA-DAè¿›è¡Œæœ€ç»ˆè¯„ä¼°
4. **é”™è¯¯åˆ†æ**: ä½¿ç”¨GEMBA-MQMè¯Šæ–­é—®é¢˜

## ğŸ“ æ”¯æŒ

- è¯¦ç»†æ–‡æ¡£: `src/metrics/README.md`
- æµ‹è¯•è„šæœ¬: `test_metrics.py`
- å®˜æ–¹ä»“åº“: https://github.com/MicrosoftTranslator/GEMBA

