"""
æœ¯è¯­å½’ä¸€åŒ–æ™ºèƒ½ä½“ - å¤„ç†é‡å¤æœ¯è¯­ï¼Œæ­£è§„åŒ–æœ¯è¯­æ ¼å¼
"""
from typing import List, Dict, Any, Optional
import logging
from dataclasses import dataclass

from ..base import BaseAgent, AgentConfig, AgentRunContext

logger = logging.getLogger(__name__)


@dataclass
class NormalizedTerm:
    """å½’ä¸€åŒ–åçš„æœ¯è¯­"""
    source_term: str
    target_term: str
    normalized_source: str
    normalized_target: str
    confidence: float
    category: str
    source_context: str
    target_context: str
    quality_score: float
    is_valid: bool
    law: str
    domain: str
    year: str
    entry_id: str
    normalization_notes: str = ""


class TermNormalizationAgent(BaseAgent):
    """æœ¯è¯­å½’ä¸€åŒ–æ™ºèƒ½ä½“"""
    
    def __init__(self, locale: str = 'zh'):
        super().__init__(AgentConfig(
            name='preprocess:bilingual-term-normalization',
            role='bilingual_terminology_normalizer',
            domain='preprocess',
            specialty='åŒè¯­æœ¯è¯­å½’ä¸€åŒ–',
            quality='review',
            locale=locale
        ))

    async def execute(self, input_data: Dict[str, Any], ctx: Optional[AgentRunContext] = None) -> List[NormalizedTerm]:
        """æ‰§è¡Œæœ¯è¯­å½’ä¸€åŒ–"""
        return await self.run(input_data, ctx)
    
    async def run(self, input_data: Dict[str, Any], context: Optional[Dict[str, Any]] = None) -> List[NormalizedTerm]:
        """è¿è¡Œæœ¯è¯­å½’ä¸€åŒ– - ä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½å½’ä¸€åŒ–ï¼ˆåŒ…å«åŒæœ¯è¯­å˜ä½“åˆå¹¶ï¼‰"""
        terms = input_data.get('terms', [])
        src_lang = input_data.get('src_lang', 'zh')
        tgt_lang = input_data.get('tgt_lang', 'en')
        batch_size = input_data.get('batch_size', 50)
        
        logger.info(f"å¼€å§‹æœ¯è¯­å½’ä¸€åŒ–ï¼Œè¾“å…¥æœ¯è¯­æ•°: {len(terms)}")
        
        if not terms:
            return []
        
        normalized_terms = []
        
        # æ‰¹é‡å¤„ç†ï¼Œä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½å½’ä¸€åŒ–ï¼ˆLLMä¼šè‡ªåŠ¨å¤„ç†åŒæœ¯è¯­çš„å˜ä½“åˆå¹¶ï¼‰
        for i in range(0, len(terms), batch_size):
            batch = terms[i:i + batch_size]
            logger.info(f"å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}/{(len(terms)-1)//batch_size + 1}: {len(batch)} ä¸ªæœ¯è¯­")
            
            # ä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½å½’ä¸€åŒ–
            batch_normalized = await self._normalize_batch_with_llm(batch, src_lang, tgt_lang)
            normalized_terms.extend(batch_normalized)
        
        logger.info(f"å½’ä¸€åŒ–å®Œæˆï¼Œè¾“å‡ºæœ¯è¯­æ•°: {len(normalized_terms)}")
        
        # âœ… ä¸å†åšè§„åˆ™å»é‡ï¼ŒLLMå·²ç»åœ¨å½’ä¸€åŒ–æ—¶å¤„ç†äº†åŒæœ¯è¯­çš„å˜ä½“åˆå¹¶
        return normalized_terms
    
    async def _normalize_batch_with_llm(self, batch: List[Dict[str, Any]], src_lang: str, tgt_lang: str) -> List[NormalizedTerm]:
        """ä½¿ç”¨LLMå¯¹æ‰¹é‡æœ¯è¯­è¿›è¡Œæ™ºèƒ½å½’ä¸€åŒ–ï¼ˆæ ¹æ®è¯­è¨€è°ƒç”¨ä¸åŒçš„å­å‡½æ•°ï¼‰"""
        # åˆ†åˆ«å½’ä¸€åŒ–æºè¯­è¨€å’Œç›®æ ‡è¯­è¨€æœ¯è¯­
        source_terms = [term.get('source_term', '') for term in batch]
        target_terms = [term.get('target_term', '') for term in batch]
        
        # æ ¹æ®è¯­è¨€è°ƒç”¨ç›¸åº”çš„å½’ä¸€åŒ–å‡½æ•°
        normalized_sources = await self._normalize_terms_by_language(source_terms, src_lang)
        normalized_targets = await self._normalize_terms_by_language(target_terms, tgt_lang)
        
        # éªŒè¯å¹¶åˆ›å»ºå½’ä¸€åŒ–æœ¯è¯­å¯¹è±¡
        normalized_terms = []
        for i, term in enumerate(batch):
            source_term = term.get('source_term', '')
            target_term = term.get('target_term', '')
            
            normalized_source = normalized_sources[i]
            normalized_target = normalized_targets[i]
            
            # éªŒè¯å½’ä¸€åŒ–ç»“æœçš„æœ‰æ•ˆæ€§
            if not self._is_valid_normalization(source_term, normalized_source, is_english=(src_lang == 'en')):
                logger.warning(f"âš ï¸ æ— æ•ˆå½’ä¸€åŒ–: '{source_term}' -> '{normalized_source}', ä½¿ç”¨åŸæœ¯è¯­")
                normalized_source = source_term
            
            if not self._is_valid_normalization(target_term, normalized_target, is_english=(tgt_lang == 'en')):
                logger.warning(f"âš ï¸ æ— æ•ˆå½’ä¸€åŒ–: '{target_term}' -> '{normalized_target}', ä½¿ç”¨åŸæœ¯è¯­")
                normalized_target = target_term
            
            normalized_terms.append(self._create_normalized_term(
                term,
                normalized_source,
                normalized_target,
                notes=""
            ))
        
        return normalized_terms
    
    async def _normalize_terms_by_language(self, terms: List[str], lang: str) -> List[str]:
        """æ ¹æ®è¯­è¨€ç±»å‹è°ƒç”¨ç›¸åº”çš„å½’ä¸€åŒ–å‡½æ•°"""
        if lang == 'zh':
            return await self._normalize_chinese(terms)
        elif lang == 'en':
            return await self._normalize_english(terms)
        elif lang == 'ja':
            return await self._normalize_japanese(terms)
        else:
            logger.warning(f"æœªçŸ¥è¯­è¨€ç±»å‹: {lang}ï¼Œä½¿ç”¨é€šç”¨å½’ä¸€åŒ–")
            return await self._normalize_generic(terms, lang)
    
    async def _normalize_chinese(self, terms: List[str]) -> List[str]:
        """ä¸­æ–‡æœ¯è¯­å½’ä¸€åŒ–ï¼ˆå¤„ç†åŒæœ¯è¯­çš„å˜ä½“ï¼Œä¸åˆå¹¶ä¸åŒæœ¯è¯­ï¼‰"""
        if not terms:
            return []
        
        terms_text = "\n".join([f"{i+1}. {term}" for i, term in enumerate(terms)])
        
        messages = [
            {
                "role": "system",
                "content": """ä½ æ˜¯ä¸“ä¸šçš„ä¸­æ–‡æ³•å¾‹æœ¯è¯­å½’ä¸€åŒ–ä¸“å®¶ï¼Œä¸“æ³¨äºä¸ºæ³•å¾‹æœ¯è¯­è¯å…¸æä¾›å‡†ç¡®ã€è§„èŒƒçš„æœ¯è¯­å¤„ç†ã€‚

**ä¸­æ–‡æ³•å¾‹æœ¯è¯­å½’ä¸€åŒ–è§„åˆ™**ï¼š

1. **ç¹ç®€ç»Ÿä¸€**ï¼šå°†æ‰€æœ‰æœ¯è¯­ç»Ÿä¸€ä¸ºç®€ä½“ä¸­æ–‡ã€‚
   - ä¾‹å¦‚ï¼š"å”è­°" â†’ "åè®®"

2. **æ ¼å¼æ¸…ç†**ï¼šä»…ç§»é™¤æœ¯è¯­å‰åå¤šä½™ç©ºæ ¼ï¼Œä¿ç•™å†…éƒ¨ç©ºæ ¼å’Œæ‰€æœ‰æ ‡ç‚¹ç¬¦å·ã€‚
   - ä¾‹å¦‚ï¼š" åˆ åŒ " â†’ "åˆåŒ"

3. **é”™åˆ«å­—æ ¡æ­£**ï¼šæ ¹æ®æƒå¨æ³•å¾‹æ–‡æœ¬æ ¡æ­£å¸¸è§é”™åˆ«å­—æˆ–å¼‚ä½“å­—ã€‚
   - ä¾‹å¦‚ï¼š"å…¶å®ƒ" â†’ "å…¶ä»–"ï¼Œ"å¸æˆ·" â†’ "è´¦æˆ·"

4. **å…¨ç§°ç®€ç§°ç»Ÿä¸€**ï¼šå¯¹äºæœ‰å…¨ç§°å’Œç®€ç§°çš„æœ¯è¯­ï¼Œç»Ÿä¸€ä½¿ç”¨å…¨ç§°ã€‚
   - ä¾‹å¦‚ï¼š"æœ‰é™å…¬å¸" â†’ "æœ‰é™è´£ä»»å…¬å¸"

5. **ğŸ”¥ ç»“æ„æ€§æ ‡è®°å½’ä¸€åŒ– - é‡è¦è§„åˆ™**ï¼š
   - **å°†æ³•æ¡ç¼–å·ã€ç« èŠ‚ç¼–å·ä¸­çš„å…·ä½“æ•°å­—ç»Ÿä¸€æ›¿æ¢ä¸ºXX**ï¼š
     - ä¾‹å¦‚ï¼š"ç¬¬36æ¡" â†’ "ç¬¬XXæ¡"
     - ä¾‹å¦‚ï¼š"ç¬¬ä¸‰åå…­æ¡" â†’ "ç¬¬XXæ¡"
     - ä¾‹å¦‚ï¼š"ç¬¬40æ¡ç¬¬ä¸€é¡¹" â†’ "ç¬¬XXæ¡ç¬¬XXé¡¹"
     - ä¾‹å¦‚ï¼š"ç¬¬87æ¡" â†’ "ç¬¬XXæ¡"
     - ä¾‹å¦‚ï¼š"ç¬¬äºŒç« " â†’ "ç¬¬XXç« "
     - ä¾‹å¦‚ï¼š"ç¬¬äº”èŠ‚" â†’ "ç¬¬XXèŠ‚"
     - ä¾‹å¦‚ï¼š"ï¼ˆä¸€ï¼‰" â†’ "ï¼ˆXXï¼‰"
     - ä¾‹å¦‚ï¼š"ï¼ˆäºŒï¼‰" â†’ "ï¼ˆXXï¼‰"
   - è¿™æ ·å¯ä»¥å°†æ‰€æœ‰ç›¸åŒç±»å‹çš„ç»“æ„æ€§æ ‡è®°ç»Ÿä¸€å½’ä¸€åŒ–ï¼Œä¾¿äºå»é‡å’Œç®¡ç†

6. **ğŸ”¥ ä¸è¦åˆå¹¶ä¸åŒæœ¯è¯­ - å…³é”®è§„åˆ™**ï¼š
   - **ä¸è¦åˆå¹¶ä¸åŒçš„æœ¯è¯­**ï¼Œå³ä½¿å®ƒä»¬æ„æ€ç›¸è¿‘ï¼š
     - âŒ é”™è¯¯ï¼šåˆå¹¶"å·¥ä¼š"å’Œ"åŠ³å·¥ç»„ç»‡" â†’ è¿™æ˜¯ä¸¤ä¸ªä¸åŒçš„æœ¯è¯­ï¼
     - âŒ é”™è¯¯ï¼šåˆå¹¶"åˆåŒ"å’Œ"åè®®" â†’ è¿™æ˜¯ä¸¤ä¸ªä¸åŒçš„æœ¯è¯­ï¼
     - âŒ é”™è¯¯ï¼šåˆå¹¶"å¾‹å¸ˆ"å’Œ"æ³•å¾‹é¡¾é—®" â†’ è¿™æ˜¯ä¸¤ä¸ªä¸åŒçš„æœ¯è¯­ï¼

7. **ç¦æ­¢åˆ è¯**ï¼šç»ä¸åˆ é™¤"çš„"ã€"ä¹‹"ç­‰åŠ©è¯ï¼Œä»¥å…å½±å“æ³•å¾‹è¯­ä¹‰ã€‚

**é‡è¦ - ä¸€ä¸€å¯¹åº”å…³ç³»**ï¼š
- **æ¯ä¸ªè¾“å…¥æœ¯è¯­å¿…é¡»æœ‰ä¸”ä»…æœ‰ä¸€ä¸ªè¾“å‡ºæœ¯è¯­**
- è¾“å‡ºæ•°é‡å¿…é¡»ç­‰äºè¾“å…¥æ•°é‡
- ç‹¬ç«‹å¤„ç†æ¯ä¸ªæœ¯è¯­
- ç¤ºä¾‹ï¼šå¦‚æœè¾“å…¥æœ‰"æœ‰é™å…¬å¸"å’Œ"æœ‰é™è´£ä»»å…¬å¸"ï¼Œè¾“å‡ºåº”è¯¥éƒ½æ˜¯"æœ‰é™è´£ä»»å…¬å¸"ï¼ˆå»é‡åœ¨åç»­é˜¶æ®µå¤„ç†ï¼‰

è¿”å›JSONæ ¼å¼ï¼š
{
    "normalized": ["æœ¯è¯­1", "æœ¯è¯­2", ...]
}"""
            },
            {
                "role": "user",
                "content": f"""è¯·å½’ä¸€åŒ–ä»¥ä¸‹{len(terms)}ä¸ªä¸­æ–‡æ³•å¾‹æœ¯è¯­ï¼š

å…³é”®è¦æ±‚ï¼š
- ç‹¬ç«‹å¤„ç†æ¯ä¸ªæœ¯è¯­
- è¾“å‡ºæ•°é‡å¿…é¡»ç­‰äºè¾“å…¥æ•°é‡ï¼ˆ{len(terms)}ä¸ªæœ¯è¯­ï¼‰
- è½¬æ¢ç¹ä½“ä¸ºç®€ä½“ï¼Œç»Ÿä¸€ç®€ç§°ä¸ºå…¨ç§°
- ä¸è¦åˆå¹¶ä¸åŒçš„æœ¯è¯­ï¼ˆå¦‚"å·¥ä¼š"å’Œ"åŠ³å·¥ç»„ç»‡"æ˜¯ä¸åŒæœ¯è¯­ï¼‰

{terms_text}

è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¿”å›æ°å¥½{len(terms)}ä¸ªå½’ä¸€åŒ–åçš„æœ¯è¯­ã€‚"""
            }
        ]
        
        try:
            result = await self.call_llm_json(messages)
            return self._parse_normalized_result(result, terms)
        except Exception as e:
            logger.error(f"ä¸­æ–‡å½’ä¸€åŒ–å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸæœ¯è¯­")
            return terms
    
    async def _normalize_english(self, terms: List[str]) -> List[str]:
        """è‹±æ–‡æœ¯è¯­å½’ä¸€åŒ–ï¼ˆå¤„ç†åŒæœ¯è¯­çš„å˜ä½“ï¼Œä¸åˆå¹¶ä¸åŒæœ¯è¯­ï¼‰"""
        if not terms:
            return []
        
        terms_text = "\n".join([f"{i+1}. {term}" for i, term in enumerate(terms)])
        
        messages = [
            {
                "role": "system",
                "content": """You are a professional legal terminology normalization expert, tasked with preparing terms for a legal dictionary.

**English Legal Terminology Normalization Rules**:

1.  **ğŸ”¥ Plural/Singular Normalization - CRITICAL RULE**:
    *   **If the input term is in plural form**, normalize it to the singular form and output as "singular/plural" (using a slash to connect both forms):
        *   Example: `"contracts"` â†’ `"contract/contracts"`
        *   Example: `"trade unions"` â†’ `"trade union/trade unions"`
        *   Example: `"attorneys"` â†’ `"attorney/attorneys"`
        *   Example: `"parties"` â†’ `"party/parties"`
        *   Example: `"companies"` â†’ `"company/companies"`
    *   **Handle compound terms** similarly:
        *   Example: `"employment contracts"` â†’ `"employment contract/employment contracts"`
        *   Example: `"criminal defendants"` â†’ `"criminal defendant/criminal defendants"`
    *   **For irregular plurals**, apply the same logic:
        *   Example: `"children"` â†’ `"child/children"`
        *   Example: `"men"` â†’ `"man/men"`
    *   **If the input term is in singular form**, output it as is (do not add plural form):
        *   Example: `"contract"` â†’ `"contract"`
        *   Example: `"attorney"` â†’ `"attorney"`
    *   **Exception**: For fixed legal terms or proper nouns that are inherently plural, keep them as is without adding singular form:
        *   Example: `"United States"` â†’ `"United States"`
        *   Example: `"Securities and Exchange Commission"` â†’ `"Securities and Exchange Commission"`
        *   Example: `"civil rights"` â†’ `"civil rights"`

2.  **ğŸ”¥ Verb Tense Normalization - CRITICAL RULE**:
    *   **Convert all verbs to their base form (infinitive without "to")**:
        *   Example: `"terminated"` â†’ `"terminate"`
        *   Example: `"terminating"` â†’ `"terminate"`
        *   Example: `"terminates"` â†’ `"terminate"`
        *   Example: `"applied"` â†’ `"apply"`
        *   Example: `"applying"` â†’ `"apply"`
        *   Example: `"executed"` â†’ `"execute"`
        *   Example: `"executing"` â†’ `"execute"`
    *   **For verb phrases**, normalize the verb to base form:
        *   Example: `"being terminated"` â†’ `"be terminated"` (or better: just `"terminate"` if it's a legal term)
        *   Example: `"has been executed"` â†’ `"execute"`
    *   **Note**: If the term is a noun derived from a verb (gerund used as noun), keep the gerund form:
        *   Example: `"termination"` â†’ `"termination"` (this is a noun, not a verb)
        *   Example: `"execution"` â†’ `"execution"` (this is a noun, not a verb)

3.  **ğŸ”¥ Structural Markers Normalization - IMPORTANT RULE**:
    *   **Replace specific numbers in article/chapter references with XX**:
        *   Example: `"Article 36"` â†’ `"Article XX"`
        *   Example: `"Article 38"` â†’ `"Article XX"`
        *   Example: `"Section 5"` â†’ `"Section XX"`
        *   Example: `"Chapter 3"` â†’ `"Chapter XX"`
        *   Example: `"Paragraph 2"` â†’ `"Paragraph XX"`
        *   Example: `"Item (1)"` â†’ `"Item (XX)"`
        *   Example: `"(a)"` â†’ `"(XX)"`
    *   This unifies all structural markers for easier deduplication

4.  **Case Handling**:
    *   Convert purely generic legal terms to lowercase.
        *   Example: `"Contract"` â†’ `"contract"`, `"Tort"` â†’ `"tort"`
    *   **Preserve the capitalization** of proper nouns, established legal doctrines, and official names.
        *   Example: `"Supreme Court"`, `"Due Process Clause"`, `"Miranda rights"`

4.  **Whitespace & Format Cleaning**: Remove leading/trailing whitespace, normalize internal spacing (e.g., collapse multiple spaces to one).
    *   Example: `"  contract  "` â†’ `"contract"`, `"employment   contract"` â†’ `"employment contract"`

5.  **Spelling Variants**:
    *   Standardize to predominant American English form (unless specified otherwise):
        *   Example: `"judgement"` â†’ `"judgment"`
        *   Example: `"colour"` â†’ `"color"`

6.  **ğŸ”¥ DO NOT Merge Different Terms - CRITICAL RULE**:
    *   **DO NOT merge different terms**, even if similar or synonymous. Treat each term independently:
        *   âŒ Wrong: Merge `"agreement"` and `"contract"` â†’ Keep both as separate terms!
        *   âŒ Wrong: Merge `"lawyer"` and `"attorney"` â†’ Keep both as separate terms!
        *   âŒ Wrong: Merge `"mediator"` and `"arbitrator"` â†’ Keep both as separate terms!

**IMPORTANT - One-to-One Mapping**:
- **Each input term MUST have exactly one output term**
- Output count MUST equal input count
- Process each term independently (de-duplication happens later in the dictionary process)
- Example: If input has both "contract" and "contracts", output will be "contract" and "contract/contracts" respectively

Return JSON format:
{
    "normalized": ["term1", "term2", ...]
}"""
            },
            {
                "role": "user",
                "content": f"""Please normalize the following {len(terms)} English legal terms:

CRITICAL REQUIREMENTS:
- Process each term independently
- Output count MUST equal input count ({len(terms)} terms)
- Convert plurals to "singular/plural" format
- Convert verb tenses to base form (infinitive)
- Replace numbers in structural markers with XX (e.g., "Article 36" â†’ "Article XX")
- DO NOT merge different terms

{terms_text}

Return strictly in JSON format with exactly {len(terms)} normalized terms."""
            }
        ]
        
        try:
            result = await self.call_llm_json(messages)
            return self._parse_normalized_result(result, terms)
        except Exception as e:
            logger.error(f"è‹±æ–‡å½’ä¸€åŒ–å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸæœ¯è¯­")
            return terms
    
    async def _normalize_japanese(self, terms: List[str]) -> List[str]:
        """æ—¥æ–‡æœ¯è¯­å½’ä¸€åŒ–"""
        if not terms:
            return []
        
        terms_text = "\n".join([f"{i+1}. {term}" for i, term in enumerate(terms)])
        
        messages = [
            {
                "role": "system",
                "content": """ã‚ãªãŸã¯æ—¥æœ¬èªæ³•å¾‹ç”¨èªã®æ­£è¦åŒ–å°‚é–€å®¶ã§ã™ã€‚æ³•å¾‹è¾å…¸æ§‹ç¯‰ã‚’ç›®çš„ã¨ã—ã¦ã€æ­£ç¢ºã§è¦ç¯„çš„ãªç”¨èªå‡¦ç†ã‚’è¡Œã„ã¾ã™ã€‚

**æ—¥æœ¬èªæ³•å¾‹ç”¨èªæ­£è¦åŒ–ãƒ«ãƒ¼ãƒ«**ï¼š

1. **è¡¨è¨˜çµ±ä¸€**ï¼š
   - ã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠè¡¨è¨˜ã¯ã€æ¨©å¨ã‚ã‚‹æ³•å¾‹æ–‡çŒ®ï¼ˆå…­æ³•å…¨æ›¸ã€åˆ¤ä¾‹é›†ç­‰ï¼‰ã«åŸºã¥ãæ¨™æº–æ¼¢å­—è¡¨è¨˜ã«çµ±ä¸€ã—ã¾ã™ã€‚
     - ä¾‹ï¼šã€Œã‘ã„ã‚„ãã€ â†’ ã€Œå¥‘ç´„ã€
   - **ãŸã ã—**ã€æ³•å¾‹ä¸Šã§ç¢ºå®šã—ãŸã‚«ã‚¿ã‚«ãƒŠèªã¯ãã®ã¾ã¾ä¿æŒã—ã¾ã™ã€‚
     - ä¾‹ï¼šã€Œãƒãƒ³ã‚³ãƒ³ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æ¡é …ã€ã¯å¤‰æ›´ã—ãªã„

2. **é€ã‚Šä»®åçµ±ä¸€**ï¼š
   - å†…é–£å‘Šç¤ºã€Œé€ã‚Šä»®åã®ä»˜ã‘æ–¹ã€åŠã³æ³•å¾‹åˆ†é‡ã®æ…£ä¾‹ã«å¾“ã£ã¦çµ±ä¸€ã—ã¾ã™ã€‚
     - ä¾‹ï¼šã€Œã†ã‘ã¨ã‚Šã€ â†’ ã€Œå—é ˜ã€ã€ã€Œã†ã‘ã‚ãŸã—ã€ â†’ ã€Œå—æ¸¡ã€

3. **èª¤å­—ãƒ»ç•°ä½“å­—ä¿®æ­£**ï¼š
   - æ˜ã‚‰ã‹ãªèª¤å­—ã‚„ç•°ä½“å­—ã‚’æ¨™æº–å½¢ã«ä¿®æ­£ã—ã¾ã™ã€‚
     - ä¾‹ï¼šã€Œå…¶ä»–ã€ â†’ ã€Œãã®ä»–ã€ã€ã€Œå¼å„Ÿã€ â†’ ã€Œå¼å„Ÿã€ï¼ˆã€Œè³ å„Ÿã€ã¨æ„å‘³ãŒç•°ãªã‚‹ãŸã‚æ³¨æ„ï¼‰

4. **ç•¥èªã¨æ­£å¼åç§°**ï¼š
   - ç•¥èªã¯æ­£å¼åç§°ã«çµ±ä¸€ã—ã¾ã™ãŒã€ä¸¡æ–¹ãŒåˆ¥å€‹ã®ç”¨èªã¨ã—ã¦èªè­˜ã•ã‚Œã‚‹å ´åˆã¯ä¿æŒã—ã¾ã™ã€‚
     - ä¾‹ï¼šã€Œæ°‘è¨´ã€ â†’ ã€Œæ°‘äº‹è¨´è¨Ÿæ³•ã€
     - ä¾‹ï¼šã€Œä¼šç¤¾æ³•ã€ã¨ã€Œä¼šç¤¾æ³•æ–½è¡Œè¦å‰‡ã€ã¯åˆ¥ã€…ã®ç”¨èªã¨ã—ã¦ä¿æŒ

5. **ğŸ”¥ æ§‹é€ çš„ãƒãƒ¼ã‚«ãƒ¼ã®æ­£è¦åŒ– - é‡è¦ãƒ«ãƒ¼ãƒ«**ï¼š
   - **æ¡æ–‡ç•ªå·ã‚„ç« ç¯€ç•ªå·ã®å…·ä½“çš„ãªæ•°å­—ã‚’XXã«çµ±ä¸€ã—ã¾ã™**ï¼š
     - ä¾‹ï¼šã€Œç¬¬36æ¡ã€ â†’ ã€Œç¬¬XXæ¡ã€
     - ä¾‹ï¼šã€Œç¬¬ä¸‰åå…­æ¡ã€ â†’ ã€Œç¬¬XXæ¡ã€
     - ä¾‹ï¼šã€Œç¬¬40æ¡ç¬¬1é …ã€ â†’ ã€Œç¬¬XXæ¡ç¬¬XXé …ã€
     - ä¾‹ï¼šã€Œç¬¬2ç« ã€ â†’ ã€Œç¬¬XXç« ã€
     - ä¾‹ï¼šã€Œç¬¬5ç¯€ã€ â†’ ã€Œç¬¬XXç¯€ã€
     - ä¾‹ï¼šã€Œï¼ˆä¸€ï¼‰ã€ â†’ ã€Œï¼ˆXXï¼‰ã€
     - ä¾‹ï¼šã€Œï¼ˆäºŒï¼‰ã€ â†’ ã€Œï¼ˆXXï¼‰ã€
   - ã“ã‚Œã«ã‚ˆã‚ŠåŒç¨®ã®æ§‹é€ çš„ãƒãƒ¼ã‚«ãƒ¼ã‚’çµ±ä¸€ã—ã€é‡è¤‡æ’é™¤ãŒå®¹æ˜“ã«ãªã‚Šã¾ã™

6. **åŒç¾©èªå‡¦ç† - é‡è¦ãƒ«ãƒ¼ãƒ«**ï¼š
   - **åŸå‰‡ã¨ã—ã¦åŒç¾©èªã¯çµ±ä¸€ã—ã¾ã›ã‚“**ã€‚æ³•å¾‹ä¸Šã€å¾®å¦™ãªæ„å‘³ã®é•ã„ãŒã‚ã‚‹ãŸã‚ã§ã™ã€‚
     - ä¾‹ï¼šã€Œå¼è­·å£«ã€ã¨ã€Œå¼è­·äººã€ã¯çµ±ä¸€ã—ãªã„ï¼ˆã€Œå¼è­·äººã€ã¯åˆ‘äº‹äº‹ä»¶ã«ç‰¹åŒ–ï¼‰
     - ä¾‹ï¼šã€Œå¥‘ç´„ã€ã¨ã€Œåˆæ„ã€ã¯çµ±ä¸€ã—ãªã„ï¼ˆæ³•çš„åŠ¹åŠ›ãŒç•°ãªã‚‹ï¼‰
   - çµ±ä¸€ã™ã‚‹ã®ã¯ã€å®Œå…¨ã«åŒç¾©ã§ä¸”ã¤ä¸€æ–¹ãŒæ¨™æº–å½¢ã¨æ˜ç¢ºã«åˆ¤æ–­ã§ãã‚‹å ´åˆã®ã¿ã§ã™ã€‚

7. **å‰Šé™¤ç¦æ­¢**ï¼š
   - ã€Œã®ã€ã€ã€Œã“ã¨ã€ãªã©ã®åŠ©è©ã‚„ã€å†—é•·ã«è¦‹ãˆã‚‹è¡¨ç¾ã‚‚çµ¶å¯¾ã«å‰Šé™¤ã—ã¾ã›ã‚“ã€‚
     - ä¾‹ï¼šã€Œå¥‘ç´„ã®è§£é™¤ã€ã‚’ã€Œå¥‘ç´„è§£é™¤ã€ã«çŸ­ç¸®ã—ãªã„

**é‡è¦**ï¼šæ­£è¦åŒ–ã«ç–‘ç¾©ãŒã‚ã‚‹å ´åˆã€ç‰¹ã«åŒç¾©èªã‚„è¡¨è¨˜ã®åˆ¤æ–­ã«è¿·ã†å ´åˆã¯ã€**å¿…ãšå…ƒã®ç”¨èªã‚’ä¿æŒã—ã¦ãã ã•ã„**ã€‚æ³•å¾‹ç”¨èªã®å®Œå…¨æ€§ã¨æ­£ç¢ºæ€§ãŒæœ€å„ªå…ˆã§ã™ã€‚

JSONå½¢å¼ã§è¿”ç­”ï¼š
{
    "normalized": ["ç”¨èª1", "ç”¨èª2", ...]
}"""
            },
            {
                "role": "user",
                "content": f"""ä»¥ä¸‹ã®{len(terms)}å€‹ã®æ—¥æœ¬èªæ³•å¾‹ç”¨èªã‚’æ­£è¦åŒ–ã—ã¦ãã ã•ã„ï¼š

{terms_text}

JSONå½¢å¼ã§å³å¯†ã«è¿”ç­”ã—ã¦ãã ã•ã„ã€‚æ•°ã¯å…¥åŠ›ã¨ä¸€è‡´ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚"""
            }
        ]
        
        try:
            result = await self.call_llm_json(messages)
            return self._parse_normalized_result(result, terms)
        except Exception as e:
            logger.error(f"æ—¥æ–‡å½’ä¸€åŒ–å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸæœ¯è¯­")
            return terms
    
    async def _normalize_generic(self, terms: List[str], lang: str) -> List[str]:
        """é€šç”¨å½’ä¸€åŒ–ï¼ˆç”¨äºå…¶ä»–è¯­è¨€ï¼‰"""
        if not terms:
            return []
        
        terms_text = "\n".join([f"{i+1}. {term}" for i, term in enumerate(terms)])
        
        messages = [
            {
                "role": "system",
                "content": f"""You are a legal terminology normalization expert for {lang}.

**Normalization Rules**:
1. Format cleaning: Remove extra spaces and standardize punctuation
2. Preserve meaning: Normalized term must be highly related to original (at least 80% overlap)
3. Standard form: Use the most standard expression

**Important**: If unable to normalize, return the original term!

Return JSON format:
{{
    "normalized": ["term1", "term2", ...]
}}"""
            },
            {
                "role": "user",
                "content": f"""Please normalize the following {len(terms)} legal terms:

{terms_text}

Return strictly in JSON format, the count must match the input."""
            }
        ]
        
        try:
            result = await self.call_llm_json(messages)
            return self._parse_normalized_result(result, terms)
        except Exception as e:
            logger.error(f"{lang}å½’ä¸€åŒ–å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸæœ¯è¯­")
            return terms
    
    def _parse_normalized_result(self, result: Dict[str, Any], original_terms: List[str]) -> List[str]:
        """è§£æLLMè¿”å›çš„å½’ä¸€åŒ–ç»“æœï¼ˆè¦æ±‚ä¸€ä¸€å¯¹åº”ï¼‰"""
        if 'error' in result:
            logger.warning(f"LLMè¿”å›é”™è¯¯: {result['error']}")
            return original_terms
        
        if 'raw' in result:
            import json as json_lib
            try:
                result = json_lib.loads(result['raw'])
            except json_lib.JSONDecodeError as e:
                logger.warning(f"JSONè§£æå¤±è´¥: {e}")
                return original_terms
        
        normalized_list = result.get('normalized', [])
        
        if not normalized_list:
            logger.warning(f"è¿”å›ä¸ºç©ºåˆ—è¡¨")
            return original_terms
        
        # âœ… è¦æ±‚è¾“å…¥è¾“å‡ºæ•°é‡ä¸¥æ ¼ä¸€è‡´
        if len(normalized_list) != len(original_terms):
            logger.warning(f"âš ï¸ è¿”å›æ•°é‡ä¸åŒ¹é…: æœŸæœ›{len(original_terms)}, å®é™…{len(normalized_list)}ï¼Œä½¿ç”¨åŸæœ¯è¯­")
            return original_terms
        
        return normalized_list
    
    def _fallback_normalize_batch(self, batch: List[Dict[str, Any]], src_lang: str, tgt_lang: str) -> List[NormalizedTerm]:
        """åå¤‡æ–¹æ¡ˆï¼šä½¿ç”¨è§„åˆ™è¿›è¡Œæ ¼å¼å½’ä¸€åŒ–"""
        normalized_terms = []
        for term in batch:
            source_term = term.get('source_term', '')
            target_term = term.get('target_term', '')
            
            normalized_source = self._normalize_term_format(source_term, is_english=False)
            normalized_target = self._normalize_term_format(target_term, is_english=(tgt_lang == 'en'))
            
            normalized_terms.append(self._create_normalized_term(
                term,
                normalized_source,
                normalized_target
            ))
        return normalized_terms
    
   
    # å»é‡ç”±Stage4æ ‡å‡†åŒ–é˜¶æ®µå¤„ç†
    
    def _is_valid_normalization(self, original: str, normalized: str, is_english: bool = False) -> bool:
        """éªŒè¯å½’ä¸€åŒ–ç»“æœæ˜¯å¦æœ‰æ•ˆã€‚

        è‹±æ–‡è§„åˆ™å¢å¼ºï¼š
        - æ¥å—å¤åˆå½¢å¼ "singular/plural"ï¼ˆä¾‹å¦‚ï¼šinstitution/institutionsï¼‰
        - æ¥å—å¸¸è§çš„å•å¤æ•°å˜åŒ–ï¼ˆä¾‹å¦‚ï¼šworkers -> workerï¼‰
        - æ”¾å®½ä¸ºåŸºäºåŒ…å«ã€è¯/å­—ç¬¦é‡å çš„ä¿å®ˆéªŒè¯
        ä¸­æ–‡è§„åˆ™ä¿æŒå­—ç¬¦é‡å â‰¥30%ã€‚
        """
        if not original or not normalized:
            return False
        
        # å®Œå…¨ç›¸åŒ
        if original == normalized:
            return True
        
        # ğŸ”¥ ç‰¹æ®Šè§„åˆ™ï¼šç»“æ„æ€§æ ‡è®°å½’ä¸€åŒ–ï¼ˆç¬¬XXæ¡ã€ç¬¬XXç« ç­‰ï¼‰
        if not is_english:
            import re
            # æ£€æŸ¥æ˜¯å¦ä¸ºç»“æ„æ€§æ ‡è®°çš„å½’ä¸€åŒ–ï¼ˆåŸæ–‡æœ‰æ•°å­—ï¼Œå½’ä¸€åŒ–åå˜æˆXXï¼‰
            structural_pattern = re.compile(r'^ç¬¬[é›¶ã€‡â—‹ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸¤\dXx]+(?:ä¹‹[é›¶ã€‡â—‹ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸¤\dXx]+)?æ¡(?:ç¬¬[é›¶ã€‡â—‹ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸¤\dXx]+(?:é¡¹|æ¬¾))?$')
            chapter_pattern = re.compile(r'^ç¬¬[é›¶ã€‡â—‹ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸¤\dXx]+(?:ç« |èŠ‚|ç›®)$')
            enum_pattern = re.compile(r'^[ï¼ˆ(][é›¶ã€‡â—‹ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\dXx]+[)ï¼‰]$')
            
            # å¦‚æœå½’ä¸€åŒ–åæ˜¯ç»“æ„æ€§æ ‡è®°ä¸”åŒ…å«XXï¼Œåˆ™æ¥å—
            if 'XX' in normalized or 'xx' in normalized or 'Xx' in normalized:
                if structural_pattern.match(normalized) or chapter_pattern.match(normalized) or enum_pattern.match(normalized):
                    # æ£€æŸ¥åŸæ–‡ä¹Ÿæ˜¯åŒç±»ç»“æ„
                    if structural_pattern.match(original) or chapter_pattern.match(original) or enum_pattern.match(original):
                        return True
        
        if is_english:
            orig_lower = original.lower().strip()
            norm_lower = normalized.lower().strip()
            
            # 0) ç‰¹æ®Šè§„åˆ™ï¼šç»“æ„æ€§æ ‡è®°å½’ä¸€åŒ–ï¼ˆArticle XX, Section XXç­‰ï¼‰
            import re
            structural_en_pattern = re.compile(r'^(article|section|chapter|paragraph|item|clause)\s+([xX]+|\d+)$', re.IGNORECASE)
            if 'xx' in norm_lower or 'XX' in normalized:
                match_norm = structural_en_pattern.match(norm_lower)
                match_orig = structural_en_pattern.match(orig_lower)
                if match_norm and match_orig:
                    # æ£€æŸ¥ç±»å‹æ˜¯å¦ç›¸åŒï¼ˆéƒ½æ˜¯article/sectionç­‰ï¼‰
                    if match_norm.group(1).lower() == match_orig.group(1).lower():
                        return True

            # 1) å¤åˆå½¢å¼ singular/plural
            if "/" in norm_lower:
                parts = [p.strip() for p in norm_lower.split("/") if p.strip()]
                if self._matches_english_number_variants(orig_lower, parts):
                    return True

            # 2) æ‹¬å·å½¢å¼ contract(s)/company(ies) ç­‰ï¼ˆå°½é‡å…¼å®¹ï¼‰
            for marker in ["(s)", "(es)", "(ies)"]:
                if norm_lower.replace(marker, "") == orig_lower:
                    return True
                if orig_lower.replace(marker, "") == norm_lower:
                    return True

            # 3) åŸè¯ä¸å½’ä¸€åŒ–ç»“æœæ˜¯å•å¤æ•°å˜ä½“ï¼ˆçŸ­åŒ…å« + é•¿åº¦å·®é™åˆ¶ï¼‰
            if orig_lower in norm_lower or norm_lower in orig_lower:
                len_diff = abs(len(orig_lower) - len(norm_lower))
                if len_diff <= max(len(orig_lower), len(norm_lower)) * 0.5:
                    return True

            # 4) ç›´æ¥æ¯”è¾ƒå¸¸è§çš„å•å¤æ•°å˜ä½“ï¼ˆåªå¤„ç†çŸ­è¯­æœ€åä¸€ä¸ªè¯ï¼‰
            orig_variants = self._generate_english_phrase_variants(orig_lower)
            norm_variants = self._generate_english_phrase_variants(norm_lower)
            if orig_lower in norm_variants or norm_lower in orig_variants:
                return True

            # 5) è¯é‡å ï¼ˆæŒ‰ç©ºæ ¼ä¸æ–œæ æ‹†åˆ†ï¼‰ï¼Œé™ä½é˜ˆå€¼åˆ°20%
            def _tokenize(text: str) -> set:
                text = text.replace("/", " ").replace("-", " ")
                return set(filter(None, text.split()))
            orig_words = _tokenize(orig_lower)
            norm_words = _tokenize(norm_lower)
            if orig_words and norm_words:
                overlap = len(orig_words & norm_words)
                min_words = min(len(orig_words), len(norm_words))
                if overlap >= max(1, int(min_words * 0.2)):
                    return True

            # 6) å­—ç¬¦é‡å ï¼ˆå»ç©ºæ ¼ä¸è¿å­—ç¬¦ï¼‰ï¼Œé˜ˆå€¼50%
            oc = set(orig_lower.replace(" ", "").replace("-", ""))
            nc = set(norm_lower.replace(" ", "").replace("-", ""))
            if oc and nc:
                overlap = len(oc & nc)
                min_chars = min(len(oc), len(nc))
                if overlap >= min_chars * 0.5:
                    return True

            return False
        else:
            # ä¸­æ–‡ï¼šæ£€æŸ¥å­—ç¬¦é‡å ï¼ˆâ‰¥80%ï¼‰
            orig_chars = set(original)
            norm_chars = set(normalized)
            overlap = len(orig_chars & norm_chars)
            min_chars = min(len(orig_chars), len(norm_chars))
            return overlap >= min_chars * 0.8

    def _matches_english_number_variants(self, original: str, parts: list) -> bool:
        """åˆ¤æ–­ original æ˜¯å¦ä¸ parts ä¸­ä»»ä¸€é¡¹æ„æˆå•/å¤æ•°å…³ç³»æˆ–ç›¸ç­‰ã€‚
        ä»…å¯¹çŸ­è¯­æœ€åä¸€ä¸ªè¯è¿›è¡Œå½¢æ€å˜åŒ–åˆ¤æ–­ã€‚
        """
        if not parts:
            return False
        original_variants = self._generate_english_phrase_variants(original)
        for p in parts:
            p = p.strip()
            if not p:
                continue
            if p in original_variants:
                return True
            # ä¹Ÿå°è¯•å¯¹ part ç”Ÿæˆå˜ä½“åšäº¤å‰åŒ¹é…
            part_variants = self._generate_english_phrase_variants(p)
            if original in part_variants:
                return True
        return False

    def _generate_english_phrase_variants(self, phrase: str) -> set:
        """ç”ŸæˆçŸ­è¯­çš„å¸¸è§å•/å¤æ•°å˜ä½“é›†åˆï¼ˆä»…æ”¹å˜æœ€åä¸€ä¸ªè¯ï¼‰ã€‚"""
        phrase = phrase.strip()
        if not phrase:
            return {phrase}
        words = phrase.split()
        last = words[-1]
        singular = self._singularize_english_word(last)
        plural = self._pluralize_english_word(singular)
        base = " ".join(words[:-1])
        singular_phrase = (base + " " + singular).strip()
        plural_phrase = (base + " " + plural).strip()
        return {phrase, singular_phrase, plural_phrase}

    def _singularize_english_word(self, word: str) -> str:
        """éå¸¸è½»é‡çš„è‹±æ–‡è¯å½¢è¿˜åŸï¼ˆåè¯ï¼Œå¯å‘å¼ï¼‰ã€‚"""
        w = word.strip().lower()
        if not w:
            return w
        irregular = {
            "men": "man",
            "women": "woman",
            "children": "child",
            "people": "person",
            "teeth": "tooth",
            "feet": "foot",
            "geese": "goose",
            "mice": "mouse",
            "indices": "index",
            "appendices": "appendix",
            "matrices": "matrix",
            "vertices": "vertex",
            "data": "datum",
        }
        if w in irregular:
            return irregular[w]
        if w.endswith("ies") and len(w) > 3:
            return w[:-3] + "y"
        # ä¼˜å…ˆå¤„ç†ä»¥ ch/sh ç»“å°¾çš„ es
        if w.endswith("ches") or w.endswith("shes"):
            return w[:-2]  # å»æ‰ es
        # ç±»ä¼¼ classes/processes -> class/process
        if w.endswith("sses") or w.endswith("xes") or w.endswith("zes"):
            return w[:-2]
        # å¸¸è§æƒ…å†µï¼šå»æ‰è¯å°¾çš„å•ä¸ª sï¼ˆé¿å…å»æ‰ ssï¼‰
        if w.endswith("s") and not w.endswith("ss"):
            return w[:-1]
        return w

    def _pluralize_english_word(self, word: str) -> str:
        """éå¸¸è½»é‡çš„è‹±æ–‡åè¯å¤æ•°ç”Ÿæˆï¼ˆå¯å‘å¼ï¼‰ã€‚"""
        w = word.strip().lower()
        if not w:
            return w
        irregular = {
            "man": "men",
            "woman": "women",
            "child": "children",
            "person": "people",
            "tooth": "teeth",
            "foot": "feet",
            "goose": "geese",
            "mouse": "mice",
            "index": "indices",
            "appendix": "appendices",
            "matrix": "matrices",
            "vertex": "vertices",
            "datum": "data",
        }
        if w in irregular:
            return irregular[w]
        if w.endswith("y") and len(w) > 1 and w[-2] not in "aeiou":
            return w[:-1] + "ies"
        if w.endswith("s") or w.endswith("x") or w.endswith("z") or w.endswith("ch") or w.endswith("sh"):
            return w + "es"
        return w + "s"
    
    def _normalize_term_format(self, term: str, is_english: bool = True) -> str:
        """å¯¹å•ä¸ªæœ¯è¯­è¿›è¡Œå½¢å¼æ­£è§„åŒ–ï¼ˆä¸æ”¹å˜è¯­ä¹‰ï¼Œåªæ­£è§„åŒ–æ ¼å¼ï¼‰"""
        if not term:
            return term
        
        if is_english:
            # è‹±æ–‡æ­£è§„åŒ–è§„åˆ™ï¼ˆéå¸¸ä¿å®ˆï¼ŒåªåšåŸºæœ¬æ¸…ç†ï¼‰
            normalized = term.strip()
            
            # 1. ç»Ÿä¸€å¤§å°å†™ï¼ˆè½¬ä¸ºå°å†™ï¼Œé™¤éæ˜¯ä¸“æœ‰åè¯ï¼‰
            # å¦‚æœæ•´ä¸ªè¯éƒ½æ˜¯å¤§å†™ï¼Œè½¬ä¸ºå°å†™
            if normalized.isupper():
                normalized = normalized.lower()
            # å¦‚æœæ˜¯æ··åˆå¤§å°å†™ï¼Œä¿æŒåŸæ ·ï¼ˆå¯èƒ½æ˜¯ä¸“æœ‰åè¯ï¼‰
            
            # 2. ç§»é™¤å¤šä½™ç©ºæ ¼
            normalized = ' '.join(normalized.split())
            
            # 3. æ­£è§„åŒ–å¼•å·å’Œç‰¹æ®Šå­—ç¬¦
            normalized = normalized.replace('"', '"').replace('"', '"')
            normalized = normalized.replace(''', "'").replace(''', "'")
            
            # æ³¨æ„ï¼šä¸åšå•å¤æ•°ç»Ÿä¸€ï¼
            # åŸå› ï¼šmediator, mediators, mediate, mediation æ˜¯ä¸åŒçš„è¯
            # å•å¤æ•°çš„"é€‰æ‹©"åº”è¯¥åœ¨æ­£è§„åŒ–é˜¶æ®µå®Œæˆ
            
            return normalized
        else:
            # ä¸­æ–‡æ­£è§„åŒ–è§„åˆ™
            normalized = term.strip()
            
            # 1. ç§»é™¤å¤šä½™ç©ºæ ¼
            normalized = ''.join(normalized.split())
            
            # 2. ç¹ç®€ç»Ÿä¸€ï¼ˆå¦‚æœéœ€è¦ï¼‰
            # TODO: å¯ä»¥æ·»åŠ ç¹ç®€è½¬æ¢
            
            # 3. ç§»é™¤å†—ä½™çš„åŠ©è¯ï¼ˆä¿å®ˆå¤„ç†ï¼‰
            # TODO: å¯ä»¥æ·»åŠ æ›´å¤šè§„åˆ™
            
            return normalized
    
    def _create_normalized_term(self, original_term: Dict[str, Any], normalized_source: str, normalized_target: str, notes: str = "") -> NormalizedTerm:
        """åˆ›å»ºå½’ä¸€åŒ–æœ¯è¯­å¯¹è±¡"""
        return NormalizedTerm(
            source_term=original_term['source_term'],
            target_term=original_term['target_term'],
            normalized_source=normalized_source,
            normalized_target=normalized_target,
            confidence=original_term.get('confidence', 0.0),
            category=original_term.get('category', ''),
            source_context=original_term.get('source_context', ''),
            target_context=original_term.get('target_context', ''),
            quality_score=original_term.get('quality_score', 0.0),
            is_valid=original_term.get('is_valid', True),
            law=original_term.get('law', ''),
            domain=original_term.get('domain', ''),
            year=original_term.get('year', ''),
            entry_id=original_term.get('entry_id', ''),
            normalization_notes=notes
        )
