"""
å¥æ³•ä¸€è‡´æ€§æ§åˆ¶å·¥ä½œæµ - ä¸‰æ­¥å¥æ³•åˆ†æ-ä¿®æ­£è¿‡ç¨‹
1. Bi-Extract Agent: æå–åŒè¯­å¥æ³•æ¨¡å¼
2. Evaluate Agent: è¯„ä¼°å¥æ³•ç¿»è¯‘
3. Translation Agent: åŸºäºè¯„ä¼°ç»“æœç”Ÿæˆæ”¹è¿›ç¿»è¯‘
"""
from typing import Dict, Any, List, Optional, TYPE_CHECKING
from ..agents.syntax.bi_extract import BiExtractAgent
from ..agents.syntax.syntax_evaluate import SyntaxEvaluateAgent
from ..agents.syntax.syntax_translation import SyntaxTranslationAgent
from ..agents.utils import get_global_control_config

if TYPE_CHECKING:
    from ..agents.utils import TranslationControlConfig


async def run_syntactic_workflow(
    source_text: str,
    target_text: str,
    src_lang: str,
    tgt_lang: str,
    term_table: Optional[List[Dict[str, Any]]] = None,
    verbose: bool = False,
    selection_config: Optional['TranslationControlConfig'] = None
) -> Dict[str, Any]:
    """è¿è¡Œå¥æ³•å·¥ä½œæµï¼šä¸‰æ­¥å¥æ³•åˆ†æ-ä¿®æ­£è¿‡ç¨‹
    
    Args:
        source_text: æºæ–‡æœ¬ï¼ˆåŸå§‹è¾“å…¥ï¼‰
        target_text: ç›®æ ‡æ–‡æœ¬ï¼ˆç¬¬ä¸€è½®ç¿»è¯‘ç»“æœï¼‰
        src_lang: æºè¯­è¨€
        tgt_lang: ç›®æ ‡è¯­è¨€
        term_table: æœ¯è¯­è¡¨ï¼ˆç”¨äºä¿æŠ¤æœ¯è¯­ï¼‰
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    
    Returns:
        åŒ…å«æ”¹è¿›åçš„ç¿»è¯‘å’Œå¥æ³•åˆ†æç»“æœçš„å­—å…¸
    """
    try:
        # 1) Bi-Extract Agent: æå–åŒè¯­å¥æ³•æ¨¡å¼
        if verbose:
            print(f"   ğŸ“ æå–å¥æ³•æ¨¡å¼...")
        
        bi_extract_agent = BiExtractAgent(locale=src_lang)
        patterns = await bi_extract_agent.execute({
            "source_text": source_text,
            "target_text": target_text,
            "source_lang": src_lang,
            "target_lang": tgt_lang
        }, None)
        
        if verbose:
            print(f"   æå–åˆ° {len(patterns)} ä¸ªå¥æ³•æ¨¡å¼")
            if patterns:
                print(f"\n   ã€è¯¦ç»†ã€‘å¥æ³•æ¨¡å¼:")
                for i, pattern in enumerate(patterns[:10], 1):
                    print(f"   {i}. {pattern.source_pattern} â†’ {pattern.target_pattern}")
                    print(f"      ç±»å‹: {pattern.modality_type}, ç½®ä¿¡åº¦: {pattern.confidence:.2f}")
        
        # 2) Evaluate Agent: è¯„ä¼°å¥æ³•ä¿çœŸåº¦
        evaluation = None
        if patterns:
            if verbose:
                print(f"   âœ“ è¯„ä¼°å¥æ³•ä¿çœŸåº¦...")
            
            evaluate_agent = SyntaxEvaluateAgent(locale=tgt_lang)
            evaluation = await evaluate_agent.execute({
                "source_text": source_text,
                "target_text": target_text,
                "patterns": patterns,
                "source_lang": src_lang,
                "target_lang": tgt_lang
            }, None)
            
            if verbose and evaluation:
                print(f"   è¯„ä¼°åˆ†æ•°: {evaluation.overall_score:.2f}")
                
                # æ˜¾ç¤ºå„ç»´åº¦è¯„åˆ†
                if hasattr(evaluation, 'modality_preservation'):
                    print(f"   - æƒ…æ€åŠ¨è¯å‡†ç¡®æ€§: {evaluation.modality_preservation:.2f}")
                if hasattr(evaluation, 'connective_consistency'):
                    print(f"   - è¿æ¥è¯é€»è¾‘: {evaluation.connective_consistency:.2f}")
                if hasattr(evaluation, 'conditional_logic'):
                    print(f"   - æ¡ä»¶å¥è§„èŒƒæ€§: {evaluation.conditional_logic:.2f}")
                if hasattr(evaluation, 'passive_voice_appropriateness'):
                    print(f"   - è¢«åŠ¨è¯­æ€é€‚å½“æ€§: {evaluation.passive_voice_appropriateness:.2f}")
                
                # æ˜¾ç¤ºå…·ä½“é—®é¢˜
                total_issues = 0
                if hasattr(evaluation, 'modality_issues') and evaluation.modality_issues:
                    total_issues += len(evaluation.modality_issues)
                if hasattr(evaluation, 'connective_issues') and evaluation.connective_issues:
                    total_issues += len(evaluation.connective_issues)
                if hasattr(evaluation, 'conditional_issues') and evaluation.conditional_issues:
                    total_issues += len(evaluation.conditional_issues)
                if hasattr(evaluation, 'passive_issues') and evaluation.passive_issues:
                    total_issues += len(evaluation.passive_issues)
                
                if total_issues > 0:
                    print(f"\n   ã€è¯¦ç»†ã€‘å‘ç° {total_issues} ä¸ªå…·ä½“å¥æ³•é—®é¢˜:")
                    
                    # æƒ…æ€åŠ¨è¯é—®é¢˜
                    if hasattr(evaluation, 'modality_issues') and evaluation.modality_issues:
                        print(f"   ğŸ”´ æƒ…æ€åŠ¨è¯é—®é¢˜ ({len(evaluation.modality_issues)}ä¸ª):")
                        for issue in evaluation.modality_issues[:2]:
                            print(f"      - {issue.get('source', '')} â†’ {issue.get('target', '')}: {issue.get('problem', '')}")
                    
                    # è¿æ¥è¯é—®é¢˜
                    if hasattr(evaluation, 'connective_issues') and evaluation.connective_issues:
                        print(f"   ğŸŸ¡ è¿æ¥è¯é—®é¢˜ ({len(evaluation.connective_issues)}ä¸ª):")
                        for issue in evaluation.connective_issues[:2]:
                            print(f"      - {issue.get('source', '')} â†’ {issue.get('target', '')}: {issue.get('problem', '')}")
                    
                    # æ¡ä»¶å¥é—®é¢˜
                    if hasattr(evaluation, 'conditional_issues') and evaluation.conditional_issues:
                        print(f"   ğŸŸ  æ¡ä»¶å¥é—®é¢˜ ({len(evaluation.conditional_issues)}ä¸ª):")
                        for issue in evaluation.conditional_issues[:2]:
                            print(f"      - {issue.get('problem', '')}")
                    
                    # è¢«åŠ¨è¯­æ€é—®é¢˜
                    if hasattr(evaluation, 'passive_issues') and evaluation.passive_issues:
                        print(f"   ğŸ”µ è¢«åŠ¨è¯­æ€é—®é¢˜ ({len(evaluation.passive_issues)}ä¸ª):")
                        for issue in evaluation.passive_issues[:2]:
                            print(f"      - {issue.get('problem', '')}")
                
                # æ˜¾ç¤ºå‘ç°çš„é—®é¢˜ï¼ˆæ€»ç»“ï¼‰
                if hasattr(evaluation, 'issues') and evaluation.issues:
                    print(f"\n   ã€æ€»ç»“ã€‘å¥æ³•é—®é¢˜:")
                    for i, issue in enumerate(evaluation.issues[:5], 1):
                        print(f"   {i}. {issue}")
                    if len(evaluation.issues) > 5:
                        print(f"   ... è¿˜æœ‰ {len(evaluation.issues) - 5} ä¸ªé—®é¢˜")
                
                # æ˜¾ç¤ºæ”¹è¿›å»ºè®®
                if hasattr(evaluation, 'recommendations') and evaluation.recommendations:
                    print(f"\n   ã€è¯¦ç»†ã€‘æ”¹è¿›å»ºè®®:")
                    for i, rec in enumerate(evaluation.recommendations[:3], 1):
                        print(f"   {i}. {rec}")
                    if len(evaluation.recommendations) > 3:
                        print(f"   ... è¿˜æœ‰ {len(evaluation.recommendations) - 3} æ¡å»ºè®®")
        
        # ğŸšª é—¨æ§ï¼šåŸºäºæ¨¡å¼ç½®ä¿¡åº¦å’Œè¯„ä¼°é—®é¢˜çš„ç»†ç²’åº¦é—¨æ§
        control_config = get_global_control_config()
        should_skip_refinement = False
        low_confidence_patterns = []  # BiExtractä¸­ç½®ä¿¡åº¦<0.9çš„æ¨¡å¼
        low_score_dimensions = []  # SyntaxEvalä¸­è¯„åˆ†<0.9çš„ç»´åº¦
        
        # æ­¥éª¤1ï¼šè¯†åˆ«BiExtractä¸­çš„ä½ç½®ä¿¡åº¦æ¨¡å¼ï¼ˆç½®ä¿¡åº¦<0.9ï¼‰
        if patterns:
            pattern_confidence_threshold = 0.9
            low_confidence_patterns = [
                p for p in patterns
                if hasattr(p, 'confidence') and p.confidence < pattern_confidence_threshold
            ]
            
            if low_confidence_patterns and verbose:
                print(f"\n   âš ï¸  BiExtractè¯†åˆ«å‡º {len(low_confidence_patterns)} ä¸ªä½ç½®ä¿¡åº¦æ¨¡å¼ï¼ˆ< {pattern_confidence_threshold}ï¼‰:")
                for i, p in enumerate(low_confidence_patterns[:3], 1):
                    print(f"      {i}. {p.source_pattern} â†’ {p.target_pattern} (ç½®ä¿¡åº¦: {p.confidence:.2f})")
                if len(low_confidence_patterns) > 3:
                    print(f"      ... è¿˜æœ‰ {len(low_confidence_patterns) - 3} ä¸ª")
        
        # æ­¥éª¤2ï¼šè¯†åˆ«SyntaxEvalä¸­çš„ä½åˆ†ç»´åº¦å’Œå…·ä½“é—®é¢˜ï¼ˆè¯„åˆ†<0.9ï¼‰
        if evaluation:
            eval_score_threshold = 0.9
            
            if hasattr(evaluation, 'modality_preservation') and evaluation.modality_preservation < eval_score_threshold:
                low_score_dimensions.append({
                    'dimension': 'modality',
                    'score': evaluation.modality_preservation,
                    'issues': getattr(evaluation, 'modality_issues', [])
                })
            
            if hasattr(evaluation, 'connective_consistency') and evaluation.connective_consistency < eval_score_threshold:
                low_score_dimensions.append({
                    'dimension': 'connective',
                    'score': evaluation.connective_consistency,
                    'issues': getattr(evaluation, 'connective_issues', [])
                })
            
            if hasattr(evaluation, 'conditional_logic') and evaluation.conditional_logic < eval_score_threshold:
                low_score_dimensions.append({
                    'dimension': 'conditional',
                    'score': evaluation.conditional_logic,
                    'issues': getattr(evaluation, 'conditional_issues', [])
                })
            
            if hasattr(evaluation, 'passive_voice_appropriateness') and evaluation.passive_voice_appropriateness < eval_score_threshold:
                low_score_dimensions.append({
                    'dimension': 'passive_voice',
                    'score': evaluation.passive_voice_appropriateness,
                    'issues': getattr(evaluation, 'passive_issues', [])
                })
            
            if low_score_dimensions and verbose:
                print(f"\n   âš ï¸  SyntaxEvalè¯†åˆ«å‡º {len(low_score_dimensions)} ä¸ªä½åˆ†ç»´åº¦ï¼ˆ< {eval_score_threshold}ï¼‰:")
                for dim in low_score_dimensions:
                    print(f"      - {dim['dimension']}: {dim['score']:.2f} (é—®é¢˜æ•°: {len(dim['issues'])})")
        
        # æ­¥éª¤3ï¼šé—¨æ§å†³ç­–
        if control_config and control_config.is_gating_enabled('syntax') and evaluation:
            # å¦‚æœæœ‰ä½ç½®ä¿¡åº¦æ¨¡å¼æˆ–ä½åˆ†ç»´åº¦ï¼Œéœ€è¦æ”¹è¿›
            if low_confidence_patterns or low_score_dimensions:
                if verbose:
                    print(f"\n   ğŸšª å¥æ³•é—¨æ§ï¼šå‘ç°éœ€è¦æ”¹è¿›çš„å†…å®¹")
                    if low_confidence_patterns:
                        print(f"      - {len(low_confidence_patterns)} ä¸ªä½ç½®ä¿¡åº¦æ¨¡å¼")
                    if low_score_dimensions:
                        print(f"      - {len(low_score_dimensions)} ä¸ªä½åˆ†ç»´åº¦")
                    print(f"      â†’ å°†è¿›è¡Œé’ˆå¯¹æ€§æ”¹è¿›")
                should_skip_refinement = False
            # å¦‚æœæ•´ä½“è¯„åˆ†ä¹Ÿé«˜äºé˜ˆå€¼ï¼Œä¸”æ²¡æœ‰ä½ç½®ä¿¡åº¦/ä½åˆ†é—®é¢˜ï¼Œå¯ä»¥è·³è¿‡
            elif not control_config.should_apply_syntax_modification(evaluation.overall_score):
                should_skip_refinement = True
                if verbose:
                    threshold = control_config.syntax_threshold
                    print(f"\n   ğŸšª å¥æ³•é—¨æ§ï¼šè¯„ä¼°åˆ†æ•° {evaluation.overall_score:.2f} â‰¥ {threshold}")
                    print(f"      ä¸”æ— ä½ç½®ä¿¡åº¦æ¨¡å¼ï¼Œæ— ä½åˆ†ç»´åº¦ â†’ ä¸ä¿®æ”¹")
        
        # å¦‚æœåˆ¤å®šå¯ä»¥è·³è¿‡ï¼Œç›´æ¥è¿”å›
        if should_skip_refinement:
            return {
                "patterns": [pattern.__dict__ if hasattr(pattern, '__dict__') else pattern for pattern in patterns],
                "evaluation": evaluation.__dict__ if evaluation and hasattr(evaluation, '__dict__') else None,
                "refinedText": target_text,  # ä¿æŒåŸæ–‡ä¸å˜
                "appliedCorrections": [],
                "ruleUpdates": [],
                "confidence": 1.0,
                "output": target_text,
                "candidates": None,
                "gated": True  # æ ‡è®°ä¸ºè¢«é—¨æ§ä¿æŠ¤
            }
        
        # 3) Translation Agent: åŸºäºè¯„ä¼°ç»“æœç”Ÿæˆæ”¹è¿›ç¿»è¯‘
        if verbose:
            print(f"   ğŸ¤– å¥æ³•å±‚ç¿»è¯‘...")
            if evaluation:
                control_config = get_global_control_config()
                gating_enabled = control_config and control_config.is_gating_enabled('syntax')
                if gating_enabled:
                    if low_confidence_patterns:
                        # é’ˆå¯¹æ€§æ”¹è¿›æ¨¡å¼
                        print(f"   - æ¨¡å¼: é’ˆå¯¹ {len(low_confidence_patterns)} ä¸ªä½ç½®ä¿¡åº¦æ¨¡å¼è¿›è¡Œæ”¹è¿›")
                    else:
                        # é—¨æ§å¯ç”¨æ—¶ï¼Œèƒ½æ‰§è¡Œåˆ°è¿™é‡Œè¯´æ˜åˆ†æ•°ä½äºé˜ˆå€¼
                        threshold = control_config.syntax_threshold
                        print(f"   - è¯„ä¼°åˆ†æ•°: {evaluation.overall_score:.2f} < é˜ˆå€¼ {threshold}ï¼ˆéœ€è¦æ”¹è¿›ï¼‰")
                else:
                    # é—¨æ§æœªå¯ç”¨
                    print(f"   - è¯„ä¼°åˆ†æ•°: {evaluation.overall_score:.2f}ï¼ˆé—¨æ§æœªå¯ç”¨ï¼Œæ‰§è¡Œç¿»è¯‘æ”¹è¿›ï¼‰")
            if patterns:
                total_patterns = len(patterns)
                focus_patterns = len(low_confidence_patterns) if low_confidence_patterns else total_patterns
                print(f"   - å‚è€ƒ {focus_patterns}/{total_patterns} ä¸ªå¥æ³•æ¨¡å¼è¿›è¡Œæ”¹è¿›")
            if term_table:
                print(f"   - ä¿æŠ¤ {len(term_table)} ä¸ªæœ¯è¯­")
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºå¥æ³•å±‚å¯ç”¨å€™é€‰é€‰æ‹©
        generate_candidates = False
        num_candidates = 3
        if selection_config:
            generate_candidates = selection_config.is_selection_enabled('syntax')
            num_candidates = selection_config.get_num_candidates('syntax')
        
        translation_agent = SyntaxTranslationAgent(
            locale=tgt_lang,
            generate_candidates=generate_candidates,
            num_candidates=num_candidates
        )
        
        # æ„å»ºagentè¾“å…¥ï¼Œä¼ é€’ä½ç½®ä¿¡åº¦æ¨¡å¼å’Œä½åˆ†ç»´åº¦
        agent_input = {
            "source_text": source_text,
            "target_text": target_text,
            "patterns": patterns,
            "evaluation": evaluation,
            "source_lang": src_lang,
            "target_lang": tgt_lang,
            "term_table": term_table or [],
            "low_confidence_patterns": low_confidence_patterns,  # BiExtractçš„ä½ç½®ä¿¡åº¦æ¨¡å¼
            "low_score_dimensions": low_score_dimensions  # SyntaxEvalçš„ä½åˆ†ç»´åº¦
        }
        
        # å¦‚æœæœ‰ä½ç½®ä¿¡åº¦æ¨¡å¼æˆ–ä½åˆ†ç»´åº¦ï¼Œä½¿ç”¨é’ˆå¯¹æ€§æ”¹è¿›æ¨¡å¼
        if low_confidence_patterns or low_score_dimensions:
            agent_input["focus_patterns"] = low_confidence_patterns  # ä¿æŒå…¼å®¹æ€§
            agent_input["refinement_mode"] = "targeted"  # é’ˆå¯¹æ€§æ”¹è¿›æ¨¡å¼
        
        translation_result = await translation_agent.execute(agent_input, None)
        
        # å¦‚æœç”Ÿæˆäº†å¤šä¸ªå€™é€‰ï¼Œä½¿ç”¨LLMé€‰æ‹©å™¨é€‰æ‹©æœ€ä½³
        if generate_candidates and translation_result.candidates and len(translation_result.candidates) > 1:
            if verbose:
                print(f"   ğŸ¯ LLMé€‰æ‹©å™¨ï¼šä»{len(translation_result.candidates)}ä¸ªå€™é€‰ä¸­é€‰æ‹©æœ€ä½³...")
            
            from ..agents.selector import LLMSelectorAgent
            selector = LLMSelectorAgent(locale=tgt_lang)
            
            # å‡†å¤‡ä¸Šä¸‹æ–‡ï¼ˆå¥æ³•è§„åˆ™å’Œè¯„ä¼°ä¿¡æ¯ï¼‰
            context = None
            if patterns or evaluation:
                context_lines = []
                if patterns:
                    context_lines.append(f"å¥æ³•æ¨¡å¼: {len(patterns)}ä¸ª")
                if evaluation:
                    context_lines.append(f"è¯„ä¼°åˆ†æ•°: {getattr(evaluation, 'score', 'N/A')}")
                    if hasattr(evaluation, 'issues') and evaluation.issues:
                        context_lines.append(f"å‘ç°é—®é¢˜: {len(evaluation.issues)}ä¸ª")
                context = "\n".join(context_lines)
            
            selector_result = await selector.execute({
                'source_text': source_text,
                'candidates': translation_result.candidates,
                'context': context,
                'layer_type': 'syntax'
            }, None)
            
            # æ›´æ–°ç¿»è¯‘ç»“æœ
            translation_result.refined_text = selector_result.best_candidate
            translation_result.confidence = selector_result.confidence
            
            if verbose:
                print(f"   âœ“ é€‰æ‹©ç»“æœ: å€™é€‰#{selector_result.best_candidate_index + 1}, ç½®ä¿¡åº¦: {selector_result.confidence:.2f}")
                print(f"   ç†ç”±: {selector_result.reasoning[:100]}...")
        
        if verbose:
            print(f"\n   ã€è¯¦ç»†ã€‘å¥æ³•æ”¹è¿›:")
            print(f"   æ”¹è¿›å‰: {target_text}")
            print(f"   æ”¹è¿›å: {translation_result.refined_text}")
            print(f"   ç½®ä¿¡åº¦: {translation_result.confidence:.2f}")

        return {
            "patterns": [pattern.__dict__ if hasattr(pattern, '__dict__') else pattern for pattern in patterns],
            "evaluation": evaluation.__dict__ if evaluation and hasattr(evaluation, '__dict__') else None,
            "refinedText": translation_result.refined_text,
            "appliedCorrections": translation_result.applied_corrections,
            "ruleUpdates": translation_result.rule_updates,
            "confidence": translation_result.confidence if hasattr(translation_result, 'confidence') else 1.0,
            "output": translation_result.refined_text,
            "candidates": translation_result.candidates if hasattr(translation_result, 'candidates') else None  # å€™é€‰ç¿»è¯‘åˆ—è¡¨
        }
    
    except Exception as e:
        if verbose:
            print(f"   âš ï¸  å¥æ³•å±‚ç¿»è¯‘å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
        
        return {
            "output": target_text,
            "error": str(e)
        }