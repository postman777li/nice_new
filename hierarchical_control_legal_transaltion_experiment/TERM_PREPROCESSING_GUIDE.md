# 术语批量预处理使用指南

## 概述

术语批量预处理系统在翻译实验前批量提取、去重、翻译术语并导入数据库，大幅提高翻译效率和术语一致性。

## 核心优势

✅ **效率提升 70%+**：避免重复LLM调用  
✅ **术语一致性 100%**：同一术语全局统一翻译  
✅ **智能复用**：自动查询数据库，只翻译新术语  
✅ **上下文辅助**：为每个术语提供原句上下文  
✅ **批量优化**：批量调用LLM，充分利用并发

## 系统架构

```
术语预处理流程：
┌─────────────────────────────────────────────────────────┐
│ 步骤1: 批量提取术语（MonoExtractAgent）                │
│   - 并发提取所有样本中的法律术语                        │
│   - 记录每个术语的上下文                                │
├─────────────────────────────────────────────────────────┤
│ 步骤2: 去重合并（DeduplicateAgent）                     │
│   - 完全匹配去重                                         │
│   - 统计出现频次                                         │
│   - 保留1-2个上下文示例                                 │
├─────────────────────────────────────────────────────────┤
│ 步骤3: 查询+批量翻译（BatchTranslateAgent）             │
│   - 先用SearchAgent查询数据库                           │
│   - 只翻译数据库中不存在的术语                          │
│   - 批量调用LLM（每批20个术语）                         │
├─────────────────────────────────────────────────────────┤
│ 步骤4: 导入数据库                                       │
│   - 自动导入新翻译的术语                                │
│   - 跳过已存在的术语                                    │
└─────────────────────────────────────────────────────────┘
```

## 使用方法

### 1. 仅预处理术语（推荐先运行）

```bash
python run_experiment.py \
    --test-set dataset/processed/test_set_zh_en.json \
    --preprocess-only \
    --term-db backend/terms.db \
    --max-concurrent 10
```

**说明**：
- `--preprocess-only`：只运行术语预处理，不执行翻译实验
- `--term-db`：指定术语库路径（可选，默认：backend/terms.db）
- `--max-concurrent`：提取术语的并发数（默认：10）

**输出**：
```
============================================================
术语批量预处理
============================================================
数据集样本数: 100
源语言: zh -> 目标语言: en
领域: law
术语库: backend/terms.db

------------------------------------------------------------
步骤1: 批量提取术语
------------------------------------------------------------
  进度: 10/100
  进度: 20/100
  ...
✓ 从 100 个样本中提取了 523 个术语

------------------------------------------------------------
步骤2: 去重合并术语
------------------------------------------------------------
✓ 去重后得到 156 个不重复术语
  Top 10 高频术语:
    1. 人民法院 (出现28次, 分数0.95)
    2. 诉讼代理人 (出现15次, 分数0.92)
    ...

------------------------------------------------------------
步骤3: 查询数据库 + 批量翻译
------------------------------------------------------------
  从数据库找到 45 个术语的翻译
  需要翻译 111 个新术语
  翻译批次 1/6 (20 个术语)
  ...
✓ 翻译完成: 数据库45个 + LLM111个 = 共156个

------------------------------------------------------------
步骤4: 导入到术语库
------------------------------------------------------------
✓ 成功导入 111 个新术语到数据库

============================================================
术语预处理统计
============================================================
总样本数: 100
提取术语数: 523
去重后术语数: 156
从数据库获取: 45
新翻译术语: 111
导入数据库: 111
结果文件: outputs/preprocessed_terms_1760180000.json
============================================================
```

### 2. 预处理 + 运行实验（一步完成）

```bash
python run_experiment.py \
    --test-set dataset/processed/test_set_zh_en.json \
    --preprocess \
    --ablation full \
    --max-concurrent 10
```

**说明**：
- `--preprocess`：先预处理术语，然后运行翻译实验
- 预处理完成后自动继续运行指定的消融实验

### 3. 常见使用场景

#### 场景A：新数据集，首次实验
```bash
# 步骤1：预处理术语
python run_experiment.py --test-set dataset/new_test.json --preprocess-only

# 步骤2：运行实验（术语已在数据库中）
python run_experiment.py --test-set dataset/new_test.json --ablation full
```

#### 场景B：已有术语库，只想补充新术语
```bash
# 直接运行预处理，会自动跳过已存在的术语
python run_experiment.py --test-set dataset/new_test.json --preprocess-only
```

#### 场景C：一次性完成预处理和实验
```bash
# 适合快速验证
python run_experiment.py --test-set dataset/test.json --preprocess --ablation full
```

## 核心Agent说明

### 1. MonoExtractAgent（单语术语提取）
- **复用已有**：直接使用 `src/agents/terminology/mono_extract.py`
- **功能**：从源文本中提取法律术语
- **输出**：术语列表（term, score, category）

### 2. DeduplicateAgent（术语去重）
- **文件**：`src/agents/terminology/deduplicate.py`
- **功能**：
  - 合并完全相同的术语
  - 统计出现频次
  - 保留最高分数
  - 收集上下文（1-2个示例）

### 3. BatchTranslateAgent（批量翻译）
- **文件**：`src/agents/terminology/batch_translate.py`
- **功能**：
  - 复用SearchAgent查询数据库
  - 只翻译数据库中不存在的术语
  - 批量调用LLM（默认每批20个）
  - 为每个术语提供1-2个上下文句子

### 4. TerminologyPreprocessor（协调器）
- **文件**：`src/agents/terminology/preprocess.py`
- **功能**：
  - 组织完整的预处理流程
  - 管理各个Agent的调用
  - 控制并发和批量大小
  - 导入结果到数据库

## 预处理结果文件

预处理完成后会生成JSON文件，包含：

```json
{
  "total_samples": 100,
  "total_extracted": 523,
  "deduplicated": 156,
  "from_database": 45,
  "from_llm": 111,
  "imported_to_db": 111,
  "top_terms": [
    {
      "term": "人民法院",
      "translation": "People's Court",
      "count": 28,
      "score": 0.95
    },
    ...
  ]
}
```

## 性能优化建议

1. **并发数调整**：
   - 小数据集（<100样本）：`--max-concurrent 5`
   - 中等数据集（100-500样本）：`--max-concurrent 10`（默认）
   - 大数据集（>500样本）：`--max-concurrent 20`

2. **批量翻译优化**：
   - 默认每批翻译20个术语
   - 可在代码中调整 `batch_size` 参数

3. **术语库维护**：
   - 定期检查和清理术语库
   - 对高频术语人工审核

## 常见问题

### Q: 预处理后术语库变化了，如何重新翻译？
A: 直接重新运行实验，系统会自动使用更新后的术语库。

### Q: 如何查看已导入的术语？
A: 查看预处理输出的JSON文件，或直接查询SQLite数据库：
```bash
sqlite3 backend/terms.db "SELECT * FROM terms ORDER BY created_at DESC LIMIT 20;"
```

### Q: 预处理很慢怎么办？
A: 
- 增加并发数 `--max-concurrent 20`
- 减少样本数先测试 `--samples 50`
- 检查网络和API配额

### Q: 可以跳过某些步骤吗？
A: 当前版本需要完整流程。如果只想导入现有术语表，请使用术语库的 `add_term` API。

## 技术细节

- **去重策略**：完全匹配（exact match），不处理相似术语
- **上下文数量**：每个术语保留1-2个原句作为翻译参考
- **数据库操作**：只导入新术语，跳过已存在的术语
- **错误处理**：单个术语提取/翻译失败不影响整体流程

## 下一步

预处理完成后：
1. 运行完整的消融实验：`--ablation full`
2. 评估翻译结果：`python evaluate_results.py results.json`
3. 分析翻译差异：`python analyze_translation_gaps.py results.json`

