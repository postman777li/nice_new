#!/usr/bin/env python3
"""
æ‰¹é‡å®žéªŒè¿è¡Œå™¨
"""
import asyncio
import argparse
import json
import time
from pathlib import Path
from typing import List, Dict, Any
import sys
import os

sys.path.insert(0, str(Path(__file__).parent))

# å¯¼å…¥é…ç½®
# é—¨æŽ§é˜ˆå€¼é…ç½®å·²ç§»é™¤ï¼Œç›´æŽ¥åœ¨å‘½ä»¤è¡Œå‚æ•°ä¸­å®šä¹‰é»˜è®¤å€¼

# åŠ è½½ .env æ–‡ä»¶ï¼ˆå¦‚æžœå­˜åœ¨ï¼‰
try:
    from dotenv import load_dotenv
    # å°è¯•ä»Žå¤šä¸ªä½ç½®åŠ è½½ .env æ–‡ä»¶
    env_paths = [
        Path(__file__).parent / '.env',  # é¡¹ç›®æ ¹ç›®å½•
        Path.cwd() / '.env',             # å½“å‰å·¥ä½œç›®å½•
        Path.home() / '.env'             # ç”¨æˆ·ä¸»ç›®å½•
    ]
    
    env_loaded = False
    for env_path in env_paths:
        if env_path.exists():
            load_dotenv(env_path, override=False)  # override=False è¡¨ç¤ºä¸è¦†ç›–å·²æœ‰çŽ¯å¢ƒå˜é‡
            print(f"âœ“ å·²åŠ è½½çŽ¯å¢ƒé…ç½®: {env_path}")
            env_loaded = True
            break
    
    if not env_loaded:
        # å¦‚æžœæ²¡æœ‰æ‰¾åˆ° .env æ–‡ä»¶ï¼Œå°è¯•é»˜è®¤åŠ è½½
        load_dotenv(override=False)
except ImportError:
    # å¦‚æžœæ²¡æœ‰å®‰è£… python-dotenvï¼Œç»™å‡ºæç¤ºä½†ç»§ç»­è¿è¡Œ
    print("ðŸ’¡ æç¤º: å®‰è£… python-dotenv å¯ä»¥è‡ªåŠ¨åŠ è½½ .env æ–‡ä»¶: pip install python-dotenv")

from run_translation import SimpleTranslator
from datasets import LegalDataset, TestSample
from metrics import LegalTranslationMetrics
from src.agents.utils import TranslationControlConfig, set_global_control_config, ControlConfigPresets
from src.agents.quality_assessor import QualityAssessorAgent


class ExperimentRunner:
    """ç®€åŒ–çš„å®žéªŒè¿è¡Œå™¨"""
    
    def __init__(self, output_dir: str = "outputs", max_concurrent: int = 10):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.metrics = LegalTranslationMetrics()
        self.max_concurrent = max_concurrent
        self.semaphore = asyncio.Semaphore(max_concurrent)
    
    async def run_sample(self, sample: TestSample, config: Dict[str, Any], verbose: bool = False, sample_idx: int = 0, total: int = 0, save_intermediate: bool = False, enable_quality_assessment: bool = False) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªæ ·æœ¬ï¼ˆå¸¦å¹¶å‘æŽ§åˆ¶ï¼‰"""
        async with self.semaphore:  # æŽ§åˆ¶å¹¶å‘æ•°
            translator = SimpleTranslator(config, verbose=verbose)
            
            try:
                if verbose:
                    print(f"[{sample_idx}/{total}] å¼€å§‹ç¿»è¯‘: {sample.id}")
                
                result = await translator.translate(
                    source=sample.source,
                    src_lang=sample.src_lang,
                    tgt_lang=sample.tgt_lang
                )
                
                # æ£€æŸ¥ç¿»è¯‘ç»“æžœæ˜¯å¦ä¸ºç©º
                final_text = result['final'].strip() if result['final'] else ''
                if not final_text and result['success']:
                    # ç¿»è¯‘ç»“æžœä¸ºç©ºï¼Œæ ‡è®°ä¸ºå¤±è´¥
                    result['success'] = False
                    result['error'] = 'Empty translation result'
                    if verbose:
                        print(f"[{sample_idx}/{total}] âš ï¸  ç¿»è¯‘ç»“æžœä¸ºç©º: {sample.id}")
                
                # è®¡ç®—æŒ‡æ ‡
                if result['success'] and sample.target and final_text:
                    metrics = self.metrics.calculate_all_metrics(
                        source=sample.source,
                        target=final_text,
                        reference=sample.target,
                        src_lang=sample.src_lang,
                        tgt_lang=sample.tgt_lang,
                        term_table=result['trace'].get('r1', {}).get('termTable', [])
                    )
                else:
                    metrics = {}
                
                # è´¨é‡è¯„ä¼°ï¼ˆå¦‚æžœå¯ç”¨ä¸”æœ‰å‚è€ƒè¯‘æ–‡ï¼‰
                quality_assessment = None
                if enable_quality_assessment and result['success'] and sample.target and final_text:
                    try:
                        if verbose:
                            print(f"[{sample_idx}/{total}] ðŸ“Š è¿›è¡Œè´¨é‡è¯„ä¼°...")
                        
                        assessor = QualityAssessorAgent(locale=sample.tgt_lang)
                        assessment = await assessor.execute({
                            'source_text': sample.source,
                            'translation': final_text,
                            'reference': sample.target,
                            'source_lang': sample.src_lang,
                            'target_lang': sample.tgt_lang
                        }, None)
                        
                        quality_assessment = {
                            'overall_score': assessment.overall_score,
                            'accuracy_score': assessment.accuracy_score,
                            'fluency_score': assessment.fluency_score,
                            'terminology_score': assessment.terminology_score,
                            'style_score': assessment.style_score,
                            'strengths': assessment.strengths,
                            'weaknesses': assessment.weaknesses,
                            'suggestions': assessment.suggestions,
                            'detailed_comparison': assessment.detailed_comparison
                        }
                        
                        if verbose:
                            print(f"[{sample_idx}/{total}] âœ“ è´¨é‡è¯„ä¼°å®Œæˆ: æ€»åˆ† {assessment.overall_score:.2%}")
                    
                    except Exception as e:
                        if verbose:
                            print(f"[{sample_idx}/{total}] âš ï¸  è´¨é‡è¯„ä¼°å¤±è´¥: {e}")
                        quality_assessment = {'error': str(e)}
                
                # æå–ä¸­é—´å±‚ç»“æžœï¼ˆå¦‚æžœå¼€å¯save_intermediateï¼‰
                intermediate_results = {}
                if save_intermediate and result['success']:
                    trace = result.get('trace', {})
                    
                    if verbose and save_intermediate:
                        print(f"[{sample_idx}/{total}] ðŸ’¾ æå–ä¸­é—´ç»“æžœ: traceåŒ…å« {list(trace.keys())}")
                    
                    # Round 1: æœ¯è¯­å±‚
                    if 'r1' in trace and trace['r1'].get('output'):
                        intermediate_results['round1_terminology'] = {
                            'prediction': trace['r1']['output'],
                            'terms_used': len(trace['r1'].get('termTable', [])),
                            'confidence': trace['r1'].get('confidence', 0.0)
                        }
                        if verbose:
                            print(f"  âœ“ æå–äº† round1_terminology")
                    elif 'r1' in trace and verbose:
                        print(f"  âš ï¸  r1å­˜åœ¨ä½†æ— output")
                    
                    # Round 2: å¥æ³•å±‚
                    if 'r2' in trace and trace['r2'].get('output'):
                        intermediate_results['round2_syntax'] = {
                            'prediction': trace['r2']['output'],
                            'confidence': trace['r2'].get('confidence', 0.0)
                        }
                        if verbose:
                            print(f"  âœ“ æå–äº† round2_syntax")
                    elif 'r2' in trace and verbose:
                        print(f"  âš ï¸  r2å­˜åœ¨ä½†æ— output")
                    
                    # Round 3: ç¯‡ç« å±‚
                    if 'r3' in trace and trace['r3'].get('output'):
                        intermediate_results['round3_discourse'] = {
                            'prediction': trace['r3']['output'],
                            'tm_used': trace['r3'].get('tm_used', False),
                            'confidence': trace['r3'].get('confidence', 0.0)
                        }
                        if verbose:
                            print(f"  âœ“ æå–äº† round3_discourse")
                    elif 'r3' in trace and verbose:
                        print(f"  âš ï¸  r3å­˜åœ¨ä½†æ— output")
                    
                    if intermediate_results and verbose:
                        print(f"  ðŸ’¾ ä¸­é—´ç»“æžœåŒ…å«: {list(intermediate_results.keys())}")
                    elif save_intermediate and not intermediate_results:
                        print(f"  âš ï¸  æ ·æœ¬ {sample.id}: save_intermediate=True ä½†æœªæå–åˆ°ä»»ä½•ä¸­é—´ç»“æžœ")
                
                if verbose and not save_intermediate:
                    print(f"[{sample_idx}/{total}] âœ“ å®Œæˆ: {sample.id}")
                
                result_dict = {
                    'sample_id': sample.id,
                    'source': sample.source,
                    'target': sample.target,
                    'prediction': final_text or sample.source,  # å¦‚æžœä¸ºç©ºï¼Œè¿”å›žæºæ–‡æœ¬
                    'success': result['success'],
                    'metrics': metrics,
                    'trace': result['trace'],
                    'metadata': sample.metadata,
                    **(({'error': result.get('error')}) if 'error' in result else {})
                }
                
                # æ·»åŠ è´¨é‡è¯„ä¼°ç»“æžœ
                if quality_assessment:
                    result_dict['quality_assessment'] = quality_assessment
                
                # æ·»åŠ ä¸­é—´ç»“æžœ
                if intermediate_results:
                    result_dict['intermediate'] = intermediate_results
                elif save_intermediate and result['success']:
                    # å¦‚æžœè®¾ç½®äº†save_intermediateä½†æ²¡æœ‰intermediate_resultsï¼Œæ‰“å°è­¦å‘Š
                    print(f"  âš ï¸  è­¦å‘Š: æ ·æœ¬ {sample.id} save_intermediate=True ä½†intermediate_resultsä¸ºç©º")
                
                return result_dict
                
            except Exception as e:
                print(f"âŒ æ ·æœ¬ {sample.id} å¤±è´¥: {e}")
                return {
                    'sample_id': sample.id,
                    'source': sample.source,
                    'target': sample.target,
                    'prediction': sample.source,
                    'success': False,
                    'error': str(e),
                    'metrics': {},
                    'metadata': sample.metadata
                }
    
    async def run_ablation(self, samples: List[TestSample], name: str, config: Dict[str, Any], verbose: bool = False, batch_mode: bool = True, save_intermediate: bool = False, enable_quality_assessment: bool = False) -> List[Dict[str, Any]]:
        """è¿è¡Œæ¶ˆèžå®žéªŒï¼ˆæ”¯æŒæ‰¹é‡å¹¶å‘ï¼‰"""
        print(f"\n{'='*60}")
        print(f"è¿è¡Œæ¶ˆèžå®žéªŒ: {name} - {config.get('name', name)}")
        print(f"{'='*60}")
        print(f"æ ·æœ¬æ•°: {len(samples)}")
        print(f"å±‚çº§æŽ§åˆ¶: max_rounds={config.get('max_rounds', 3)}")
        print(f"ä½¿ç”¨æœ¯è¯­åº“: {config.get('useTermBase', False)}")
        print(f"å¹¶å‘æ¨¡å¼: {'æ‰¹é‡å¹¶å‘' if batch_mode else 'é€ä¸ªå¤„ç†'} (æœ€å¤§å¹¶å‘: {self.max_concurrent})")
        if save_intermediate:
            print(f"ðŸ’¾ ä¿å­˜ä¸­é—´å±‚ç»“æžœ: æ˜¯")
        if enable_quality_assessment:
            print(f"ðŸ“Š è´¨é‡è¯„ä¼°: å¯ç”¨")
        print()
        
        if batch_mode:
            # æ‰¹é‡å¹¶å‘å¤„ç†
            print(f"ðŸš€ å¯åŠ¨æ‰¹é‡å¹¶å‘ç¿»è¯‘...")
            import time
            start_time = time.time()
            
            # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
            tasks = [
                self.run_sample(sample, config, verbose=verbose, sample_idx=i, total=len(samples), save_intermediate=save_intermediate, enable_quality_assessment=enable_quality_assessment)
                for i, sample in enumerate(samples, 1)
            ]
            
            # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # å¤„ç†å¼‚å¸¸ç»“æžœ
            processed_results = []
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    sample = samples[i]
                    print(f"âŒ æ ·æœ¬ {sample.id} å¼‚å¸¸: {result}")
                    processed_results.append({
                        'sample_id': sample.id,
                        'source': sample.source,
                        'target': sample.target,
                        'prediction': sample.source,
                        'success': False,
                        'error': str(result),
                        'metrics': {},
                        'metadata': sample.metadata
                    })
                else:
                    processed_results.append(result)
            
            elapsed = time.time() - start_time
            print(f"\nâœ“ æ‰¹é‡ç¿»è¯‘å®Œæˆï¼Œè€—æ—¶: {elapsed:.2f}ç§’")
            print(f"  å¹³å‡é€Ÿåº¦: {len(samples)/elapsed:.2f} æ¡/ç§’")
            results = processed_results
        else:
            # é€ä¸ªå¤„ç†ï¼ˆåŽŸæœ‰é€»è¾‘ï¼‰
            results = []
            for i, sample in enumerate(samples, 1):
                print(f"[{i}/{len(samples)}] å¤„ç†: {sample.id}")
                result = await self.run_sample(sample, config, verbose=verbose, sample_idx=i, total=len(samples), save_intermediate=save_intermediate, enable_quality_assessment=enable_quality_assessment)
                results.append(result)
                
                if result['success']:
                    print(f"  âœ… å®Œæˆ")
                else:
                    print(f"  âŒ å¤±è´¥: {result.get('error', 'Unknown')}")
        
        # ç»Ÿè®¡æ¯å±‚ä¿®æ”¹æƒ…å†µï¼ˆå¦‚æžœæœ‰traceä¿¡æ¯ï¼‰
        layer_modifications = {
            'r1_has_terms': 0,  # æœ¯è¯­å±‚ä½¿ç”¨äº†æœ¯è¯­
            'r1_term_count': [],  # æœ¯è¯­æ•°é‡åˆ—è¡¨
            'r1_to_r2': 0,  # å¥æ³•å±‚ä¿®æ”¹
            'r2_to_r3': 0,  # ç¯‡ç« å±‚ä¿®æ”¹
            'r1_to_r3': 0,  # æ€»ä½“ä¿®æ”¹
            'r2_gated': 0,  # å¥æ³•å±‚è¢«é—¨æŽ§
            'r3_gated': 0,  # ç¯‡ç« å±‚è¢«é—¨æŽ§
            'total_with_trace': 0
        }
        
        for r in results:
            if 'trace' in r and r['trace']:
                trace = r['trace']
                layer_modifications['total_with_trace'] += 1
                
                # æ£€æŸ¥R1æœ¯è¯­å±‚
                if 'r1' in trace:
                    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†æœ¯è¯­
                    term_table = trace['r1'].get('termTable', [])
                    if term_table and len(term_table) > 0:
                        layer_modifications['r1_has_terms'] += 1
                        layer_modifications['r1_term_count'].append(len(term_table))
                
                # æ£€æŸ¥R1->R2ä¿®æ”¹
                if 'r1' in trace and 'r2' in trace:
                    r1_out = trace['r1'].get('output', '')
                    r2_out = trace['r2'].get('output', '')
                    if r1_out and r2_out and r1_out != r2_out:
                        layer_modifications['r1_to_r2'] += 1
                    
                    # æ£€æŸ¥R2æ˜¯å¦è¢«é—¨æŽ§
                    if trace['r2'].get('gated', False):
                        layer_modifications['r2_gated'] += 1
                
                # æ£€æŸ¥R2->R3ä¿®æ”¹
                if 'r2' in trace and 'r3' in trace:
                    r2_out = trace['r2'].get('output', '')
                    r3_out = trace['r3'].get('output', '')
                    if r2_out and r3_out and r2_out != r3_out:
                        layer_modifications['r2_to_r3'] += 1
                    
                    # æ£€æŸ¥R3æ˜¯å¦è¢«é—¨æŽ§
                    if trace['r3'].get('gated', False):
                        layer_modifications['r3_gated'] += 1
                
                # æ£€æŸ¥R1->R3æ€»ä½“ä¿®æ”¹
                if 'r1' in trace and 'r3' in trace:
                    r1_out = trace['r1'].get('output', '')
                    r3_out = trace['r3'].get('output', '')
                    if r1_out and r3_out and r1_out != r3_out:
                        layer_modifications['r1_to_r3'] += 1
        
        # æ˜¾ç¤ºä¿®æ”¹ç»Ÿè®¡
        if layer_modifications['total_with_trace'] > 0:
            total = layer_modifications['total_with_trace']
            print(f"\nðŸ“Š å±‚çº§ä¿®æ”¹ç»Ÿè®¡:")
            
            # æœ¯è¯­å±‚ç»Ÿè®¡
            if layer_modifications['r1_has_terms'] > 0:
                avg_terms = sum(layer_modifications['r1_term_count']) / len(layer_modifications['r1_term_count'])
                print(f"  æœ¯è¯­å±‚(R1)ä½¿ç”¨æœ¯è¯­: {layer_modifications['r1_has_terms']}/{total} ({layer_modifications['r1_has_terms']/total*100:.1f}%)")
                print(f"    å¹³å‡æœ¯è¯­æ•°: {avg_terms:.1f} ä¸ª")
            else:
                print(f"  æœ¯è¯­å±‚(R1)ä½¿ç”¨æœ¯è¯­: 0/{total} (0.0%)")
            
            # å¥æ³•å±‚å’Œç¯‡ç« å±‚ç»Ÿè®¡
            print(f"  å¥æ³•å±‚(R1â†’R2)ä¿®æ”¹: {layer_modifications['r1_to_r2']}/{total} ({layer_modifications['r1_to_r2']/total*100:.1f}%)")
            print(f"  ç¯‡ç« å±‚(R2â†’R3)ä¿®æ”¹: {layer_modifications['r2_to_r3']}/{total} ({layer_modifications['r2_to_r3']/total*100:.1f}%)")
            print(f"  æ€»ä½“(R1â†’R3)ä¿®æ”¹: {layer_modifications['r1_to_r3']}/{total} ({layer_modifications['r1_to_r3']/total*100:.1f}%)")
            
            if layer_modifications['r2_gated'] > 0 or layer_modifications['r3_gated'] > 0:
                print(f"\nðŸšª é—¨æŽ§ç»Ÿè®¡:")
                if layer_modifications['r2_gated'] > 0:
                    print(f"  å¥æ³•å±‚è¢«é—¨æŽ§: {layer_modifications['r2_gated']}/{total} ({layer_modifications['r2_gated']/total*100:.1f}%)")
                if layer_modifications['r3_gated'] > 0:
                    print(f"  ç¯‡ç« å±‚è¢«é—¨æŽ§: {layer_modifications['r3_gated']}/{total} ({layer_modifications['r3_gated']/total*100:.1f}%)")
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        valid_results = [r for r in results if r['success'] and r['metrics']]
        if valid_results:
            avg_metrics = {}
            metric_names = ['termbase_accuracy', 'deontic_preservation', 'conditional_logic_preservation', 'comet_score']
            for metric in metric_names:
                values = [r['metrics'].get(metric, 0) for r in valid_results if metric in r['metrics']]
                if values:
                    avg_metrics[metric] = sum(values) / len(values)
            
            print(f"\n{name} å¹³å‡æŒ‡æ ‡:")
            for metric, value in avg_metrics.items():
                print(f"  {metric}: {value:.3f}")
            
            print(f"æˆåŠŸçŽ‡: {len(valid_results)}/{len(results)} ({len(valid_results)/len(results)*100:.1f}%)")
        
        return results
    
    def _clean_for_json(self, obj, seen=None):
        """æ¸…ç†å¯¹è±¡ä¸­çš„å¾ªçŽ¯å¼•ç”¨å’Œä¸å¯åºåˆ—åŒ–çš„å†…å®¹"""
        if seen is None:
            seen = set()
        
        # èŽ·å–å¯¹è±¡ID
        obj_id = id(obj)
        if obj_id in seen:
            return None  # å¾ªçŽ¯å¼•ç”¨ï¼Œè¿”å›žNone
        
        # åŸºæœ¬ç±»åž‹ç›´æŽ¥è¿”å›ž
        if obj is None or isinstance(obj, (str, int, float, bool)):
            return obj
        
        # åˆ—è¡¨
        if isinstance(obj, list):
            seen.add(obj_id)
            result = [self._clean_for_json(item, seen) for item in obj]
            seen.remove(obj_id)
            return result
        
        # å­—å…¸
        if isinstance(obj, dict):
            seen.add(obj_id)
            result = {}
            for key, value in obj.items():
                # è·³è¿‡ä¸€äº›å·²çŸ¥çš„é—®é¢˜å­—æ®µ
                if key in ['_llm_client', '_db', '_tm_db', 'config']:
                    continue
                try:
                    result[key] = self._clean_for_json(value, seen)
                except:
                    result[key] = str(value)  # åºåˆ—åŒ–å¤±è´¥å°±è½¬æˆå­—ç¬¦ä¸²
            seen.remove(obj_id)
            return result
        
        # å¯¹è±¡ï¼ˆæœ‰__dict__å±žæ€§ï¼‰
        if hasattr(obj, '__dict__'):
            seen.add(obj_id)
            result = {}
            for key, value in obj.__dict__.items():
                if key.startswith('_'):  # è·³è¿‡ç§æœ‰å±žæ€§
                    continue
                try:
                    result[key] = self._clean_for_json(value, seen)
                except:
                    result[key] = str(value)
            seen.remove(obj_id)
            return result
        
        # å…¶ä»–ç±»åž‹è½¬ä¸ºå­—ç¬¦ä¸²
        return str(obj)
    
    def save_results(self, all_results: Dict[str, List[Dict[str, Any]]]):
        """ä¿å­˜ç»“æžœ"""
        timestamp = int(time.time())
        output_file = self.output_dir / f"experiment_results_{timestamp}.json"
        
        # æ¸…ç†å¾ªçŽ¯å¼•ç”¨
        cleaned_results = self._clean_for_json(all_results)
        
        # ä¿å­˜å®Œæ•´ç»“æžœ
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(cleaned_results, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… ç»“æžœå·²ä¿å­˜åˆ°: {output_file}")
        
        # å¦‚æžœæœ‰ä»Žfullä¸­æå–çš„ä¸­é—´å±‚ç»“æžœï¼Œå•ç‹¬ä¿å­˜å®ƒä»¬
        saved_intermediate = []
        for ablation in ['terminology', 'terminology_syntax']:
            if ablation in all_results and len(all_results[ablation]) > 0:
                # æ£€æŸ¥æ˜¯å¦æ˜¯ä»Žfullæå–çš„ï¼ˆæ²¡æœ‰å®Œæ•´çš„traceå­—æ®µï¼‰
                sample = all_results[ablation][0]
                if 'trace' not in sample or not sample.get('trace'):
                    # å•ç‹¬ä¿å­˜
                    intermediate_file = self.output_dir / f"experiment_results_{timestamp}_{ablation}.json"
                    with open(intermediate_file, 'w', encoding='utf-8') as f:
                        json.dump({ablation: all_results[ablation]}, f, ensure_ascii=False, indent=2)
                    saved_intermediate.append(intermediate_file)
                    print(f"  âœ… {ablation}å±‚ç»“æžœå·²å•ç‹¬ä¿å­˜åˆ°: {intermediate_file}")
        
        if saved_intermediate:
            print(f"  ðŸ’¾ å…±ä¿å­˜äº† {len(saved_intermediate)} ä¸ªä¸­é—´å±‚ç»“æžœæ–‡ä»¶")
        
        # å¦‚æžœæœ‰è´¨é‡è¯„ä¼°ç»“æžœï¼Œå•ç‹¬ä¿å­˜
        for ablation, results in all_results.items():
            if results and len(results) > 0:
                # æ£€æŸ¥æ˜¯å¦æœ‰è´¨é‡è¯„ä¼°æ•°æ®
                has_quality_assessment = any('quality_assessment' in r and r.get('quality_assessment') for r in results)
                if has_quality_assessment:
                    # æå–è´¨é‡è¯„ä¼°æ•°æ®
                    quality_data = []
                    for r in results:
                        if 'quality_assessment' in r and r.get('quality_assessment'):
                            quality_data.append({
                                'sample_id': r.get('sample_id'),
                                'source': r.get('source'),
                                'target': r.get('target'),
                                'prediction': r.get('prediction'),
                                'quality_assessment': r['quality_assessment']
                            })
                    
                    if quality_data:
                        qa_file = self.output_dir / f"experiment_results_{timestamp}_{ablation}_quality_assessment.json"
                        with open(qa_file, 'w', encoding='utf-8') as f:
                            json.dump({
                                'ablation': ablation,
                                'total_samples': len(quality_data),
                                'samples': quality_data
                            }, f, ensure_ascii=False, indent=2)
                        print(f"  ðŸ“Š {ablation}å±‚è´¨é‡è¯„ä¼°ç»“æžœå·²å•ç‹¬ä¿å­˜åˆ°: {qa_file}")
        
        return output_file


async def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='è¿è¡Œç¿»è¯‘å®žéªŒ')
    parser.add_argument('--samples', type=int, default=0, help='æ ·æœ¬æ•°é‡ï¼ˆ0=å…¨éƒ¨ï¼Œé»˜è®¤: 0ä½¿ç”¨å®Œæ•´æµ‹è¯•é›†ï¼‰')
    parser.add_argument('--ablations', nargs='+', 
                       choices=['baseline', 'terminology', 'terminology_syntax', 'full'],
                       default=['baseline', 'full'],
                       help='è¦è¿è¡Œçš„æ¶ˆèžå®žéªŒï¼šbaseline(çº¯LLM), terminology(æœ¯è¯­), terminology_syntax(æœ¯è¯­+å¥æ³•), full(å…¨éƒ¨ä¸‰å±‚)ã€‚é»˜è®¤: baseline+full')
    parser.add_argument('--output-dir', default='outputs', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--verbose', action='store_true', help='æ˜¾ç¤ºè¯¦ç»†è¾“å‡º')
    parser.add_argument('--test-set', default='dataset/processed/test_set_zh_en.json', help='æµ‹è¯•é›†è·¯å¾„')
    parser.add_argument('--max-concurrent', type=int, default=10, help='æœ€å¤§å¹¶å‘æ•°ï¼ˆé»˜è®¤: 10ï¼‰')
    parser.add_argument('--no-batch', action='store_true', help='ç¦ç”¨æ‰¹é‡å¹¶å‘æ¨¡å¼ï¼Œé€ä¸ªå¤„ç†')
    parser.add_argument('--preprocess', action='store_true', help='é¢„å¤„ç†æœ¯è¯­ï¼šä»Žæ•°æ®é›†æå–ã€åŽ»é‡ã€ç¿»è¯‘æœ¯è¯­å¹¶å¯¼å…¥æ•°æ®åº“')
    parser.add_argument('--preprocess-only', action='store_true', help='ä»…è¿è¡Œæœ¯è¯­é¢„å¤„ç†ï¼Œä¸æ‰§è¡Œç¿»è¯‘å®žéªŒ')
    parser.add_argument('--term-db', default='terms.db', help='æœ¯è¯­åº“è·¯å¾„ï¼ˆé»˜è®¤: terms.dbï¼‰')
    parser.add_argument('--save-intermediate', action='store_true', help='ä¿å­˜ä¸­é—´å±‚ç¿»è¯‘ç»“æžœï¼ˆæœ¯è¯­å±‚ã€å¥æ³•å±‚ã€ç¯‡ç« å±‚ï¼‰ï¼Œé€‚ç”¨äºŽfullå®žéªŒ')
    
    # LLMå€™é€‰é€‰æ‹©å‚æ•°
    parser.add_argument('--selection-layers', type=str, default='none', 
                       help='å¯ç”¨LLMå€™é€‰é€‰æ‹©çš„å±‚çº§: none/last/all/discourse/terminology,syntax,discourse (é»˜è®¤: none)')
    parser.add_argument('--num-candidates', type=int, default=3,
                       help='ç”Ÿæˆçš„å€™é€‰æ•°é‡ï¼ˆé»˜è®¤: 3ï¼‰')
    
    # é—¨æŽ§å‚æ•°ï¼ˆè¾“å…¥çº§åˆ«è¿‡æ»¤ï¼‰
    parser.add_argument('--gating-layers', type=str, default='all',
                       help='å¯ç”¨é—¨æŽ§çš„å±‚çº§: none/all/terminology,syntax,discourse (é»˜è®¤: none)')
    parser.add_argument('--term-gate-threshold', type=float, default=0.8,
                       help='æœ¯è¯­ç½®ä¿¡åº¦é—¨æŽ§é˜ˆå€¼ï¼Œä½ŽäºŽæ­¤å€¼çš„æœ¯è¯­è¢«è¿‡æ»¤ï¼ˆé»˜è®¤: 0.8ï¼‰')
    parser.add_argument('--syntax-gate-threshold', type=float, default=0.85,
                       help='å¥æ³•è¯„ä¼°åˆ†æ•°é—¨æŽ§é˜ˆå€¼ï¼Œé«˜äºŽæ­¤å€¼ä¸ä¿®æ”¹ï¼ˆé»˜è®¤: 0.85ï¼‰')
    parser.add_argument('--discourse-gate-threshold', type=float, default=0.9,
                       help='ç¯‡ç« è¯„ä¼°åˆ†æ•°é—¨æŽ§é˜ˆå€¼ï¼Œé«˜äºŽæ­¤å€¼ä¸ä¿®æ”¹ï¼ˆé»˜è®¤: 0.9ï¼‰')
    parser.add_argument('--tm-gate-threshold', type=float, default=0.4,
                       help='TMç›¸ä¼¼åº¦é—¨æŽ§é˜ˆå€¼ï¼Œä½ŽäºŽæ­¤å€¼çš„TMè¢«è¿‡æ»¤ï¼ˆé»˜è®¤: 0.4ï¼‰')
    
    # è´¨é‡è¯„ä¼°å‚æ•°
    parser.add_argument('--enable-quality-assessment', action='store_true', 
                       help='å¯ç”¨è´¨é‡è¯„ä¼°ï¼šå¯¹æ¯ä¸ªç¿»è¯‘ç»“æžœè¿›è¡Œè¯¦ç»†çš„è´¨é‡è¯„ä¼°ï¼ˆéœ€è¦å‚è€ƒè¯‘æ–‡ï¼‰')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥å¿…éœ€çš„çŽ¯å¢ƒå˜é‡
    api_key = os.getenv('OPENAI_API_KEY', '').strip()
    if not api_key:
        print("\n" + "=" * 60)
        print("âŒ é”™è¯¯ï¼šæœªè®¾ç½® OPENAI_API_KEY çŽ¯å¢ƒå˜é‡")
        print("=" * 60)
        print("\nè¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¹‹ä¸€è®¾ç½® API å¯†é’¥ï¼š")
        print("\næ–¹å¼1ï¼šåˆ›å»º .env æ–‡ä»¶ï¼ˆæŽ¨èï¼‰")
        print("  åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º .env æ–‡ä»¶ï¼Œå†…å®¹å¦‚ä¸‹ï¼š")
        print("  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        print("  OPENAI_API_KEY=your-api-key-here")
        print("  OPENAI_BASE_URL=https://api.openai.com/v1  # å¯é€‰")
        print("  OPENAI_API_MODEL=gpt-4o-mini                # å¯é€‰")
        print("  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        print("\næ–¹å¼2ï¼šå‘½ä»¤è¡Œè®¾ç½®ï¼ˆä¸´æ—¶ï¼‰")
        print("  export OPENAI_API_KEY='your-api-key-here'")
        print("\næ–¹å¼3ï¼šç³»ç»ŸçŽ¯å¢ƒå˜é‡ï¼ˆæ°¸ä¹…ï¼‰")
        print("  # æ·»åŠ åˆ° ~/.bashrc æˆ– ~/.zshrc")
        print("  echo 'export OPENAI_API_KEY=your-api-key-here' >> ~/.bashrc")
        print("  source ~/.bashrc")
        print("\næ”¯æŒçš„çŽ¯å¢ƒå˜é‡ï¼š")
        print("  OPENAI_API_KEY        - OpenAI APIå¯†é’¥ï¼ˆå¿…éœ€ï¼‰")
        print("  OPENAI_BASE_URL       - è‡ªå®šä¹‰APIç«¯ç‚¹ï¼ˆå¯é€‰ï¼Œå¦‚ç«å±±å¼•æ“Žï¼‰")
        print("  OPENAI_API_MODEL      - é»˜è®¤æ¨¡åž‹ï¼ˆå¯é€‰ï¼Œé»˜è®¤: gpt-4o-miniï¼‰")
        print("  LLM_TIMEOUT           - è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆå¯é€‰ï¼Œç§’ï¼Œé»˜è®¤: 300ï¼‰")
        print("  LLM_MAX_CONCURRENT    - æœ€å¤§å¹¶å‘æ•°ï¼ˆå¯é€‰ï¼Œé»˜è®¤: 10ï¼‰")
        print("  HF_ENDPOINT           - Hugging Faceé•œåƒï¼ˆå¯é€‰ï¼Œå¦‚: https://hf-mirror.comï¼‰")
        print("=" * 60 + "\n")
        return 1
    
    print("=" * 60)
    print("æ³•å¾‹ç¿»è¯‘æ‰¹é‡å®žéªŒ")
    print("=" * 60)
    print(f"âœ“ APIå¯†é’¥: {api_key[:8]}...{api_key[-4:]}")
    if os.getenv('OPENAI_BASE_URL'):
        print(f"âœ“ APIç«¯ç‚¹: {os.getenv('OPENAI_BASE_URL')}")
    print(f"âœ“ é»˜è®¤æ¨¡åž‹: {os.getenv('OPENAI_API_MODEL', 'gpt-4o-mini')}")
    
    # åˆ›å»ºå¹¶è®¾ç½®å…¨å±€ç¿»è¯‘æŽ§åˆ¶é…ç½®
    control_config = TranslationControlConfig.from_args(
        selection_layers=args.selection_layers,
        num_candidates=args.num_candidates,
        gating_layers=args.gating_layers,
        term_threshold=args.term_gate_threshold,
        syntax_threshold=args.syntax_gate_threshold,
        discourse_threshold=args.discourse_gate_threshold,
        tm_threshold=args.tm_gate_threshold
    )
    set_global_control_config(control_config)
    
    # æ˜¾ç¤ºLLMå€™é€‰é€‰æ‹©é…ç½®
    if args.selection_layers and args.selection_layers != 'none':
        print(f"âœ“ LLMå€™é€‰é€‰æ‹©: {args.selection_layers} å±‚çº§, {args.num_candidates} ä¸ªå€™é€‰")
    else:
        print(f"âœ“ LLMå€™é€‰é€‰æ‹©: æœªå¯ç”¨")
    
    if args.gating_layers and args.gating_layers != 'none':
        print(f"âœ“ é—¨æŽ§æœºåˆ¶: {args.gating_layers} å±‚çº§")
        print(f"  - æœ¯è¯­é˜ˆå€¼: {args.term_gate_threshold}")
        print(f"  - å¥æ³•é˜ˆå€¼: {args.syntax_gate_threshold}")
        print(f"  - ç¯‡ç« é˜ˆå€¼: {args.discourse_gate_threshold}")
        print(f"  - TMé˜ˆå€¼: {args.tm_gate_threshold}")
    else:
        print(f"âœ“ é—¨æŽ§æœºåˆ¶: æœªå¯ç”¨")
    
    # æ˜¾ç¤ºè´¨é‡è¯„ä¼°é…ç½®
    if args.enable_quality_assessment:
        print(f"âœ“ è´¨é‡è¯„ä¼°: å¯ç”¨ï¼ˆå°†å¯¹æ‰€æœ‰ç¿»è¯‘ç»“æžœè¿›è¡Œè¯¦ç»†è´¨é‡è¯„ä¼°ï¼‰")
    else:
        print(f"âœ“ è´¨é‡è¯„ä¼°: æœªå¯ç”¨")
    
    # åŠ è½½çœŸå®žæµ‹è¯•é›†
    print(f"\nåŠ è½½æµ‹è¯•æ•°æ®é›†...")
    test_set_path = Path(args.test_set) if Path(args.test_set).is_absolute() else Path(__file__).parent / args.test_set
    
    if test_set_path.exists():
        import json
        with open(test_set_path, 'r', encoding='utf-8') as f:
            test_data = json.load(f)
        
        # ä»Žentrieså­—æ®µèŽ·å–æ•°æ®
        entries = test_data.get('entries', test_data if isinstance(test_data, list) else [])
        metadata = test_data.get('metadata', {})
        
        print(f"  æ•°æ®é›†ä¿¡æ¯: {metadata.get('pair', 'unknown')} - {metadata.get('total_entries', len(entries))} æ¡")
        print(f"  é¢†åŸŸ: {', '.join(metadata.get('domains', []))}")
        
        # è½¬æ¢ä¸ºTestSampleå¯¹è±¡
        all_samples = []
        for item in entries:
            sample = TestSample(
                id=str(item.get('id', len(all_samples) + 1)),
                source=item['source'],
                target=item.get('target', ''),
                src_lang='zh',  # ä»Žæ–‡ä»¶åæŽ¨æ–­
                tgt_lang='en',
                document_id=item.get('law', 'unknown'),
                article_id=str(item.get('id', '')),
                metadata={
                    'domain': item.get('domain', ''),
                    'year': item.get('year', ''),
                    'law': item.get('law', '')
                }
            )
            all_samples.append(sample)
        
        # æ ¹æ®å‚æ•°é€‰æ‹©æ ·æœ¬æ•°é‡
        if args.samples > 0 and args.samples < len(all_samples):
            samples = all_samples[:args.samples]
            print(f"  âœ“ ä»Žæµ‹è¯•é›†åŠ è½½äº† {len(samples)}/{len(all_samples)} ä¸ªæ ·æœ¬")
        else:
            samples = all_samples
            print(f"  âœ“ åŠ è½½äº†å®Œæ•´æµ‹è¯•é›†: {len(samples)} ä¸ªæ ·æœ¬")
    else:
        # å¦‚æžœæµ‹è¯•é›†ä¸å­˜åœ¨ï¼Œä½¿ç”¨ç¤ºä¾‹æ•°æ®
        print(f"  âš ï¸  æœªæ‰¾åˆ°æµ‹è¯•é›†: {test_set_path}")
        print(f"  ä½¿ç”¨ç¤ºä¾‹æ•°æ®...")
        samples = [
            TestSample(
                id=f"sample_{i}",
                source=f"è¿™æ˜¯æµ‹è¯•æ ·æœ¬{i}çš„æºæ–‡æœ¬ã€‚åŠ³åŠ¨è€…äº«æœ‰å¹³ç­‰å°±ä¸šçš„æƒåˆ©ã€‚",
                target=f"This is the source text of test sample {i}. Workers have the right to equal employment.",
                src_lang="zh",
                tgt_lang="en",
                document_id="test",
                article_id=str(i),
                metadata={}
            )
            for i in range(1, args.samples + 1)
        ]
        print(f"  åˆ›å»ºäº† {len(samples)} ä¸ªç¤ºä¾‹æ ·æœ¬")
    
    # æœ¯è¯­é¢„å¤„ç†ï¼ˆå¯é€‰ï¼‰
    if args.preprocess or args.preprocess_only:
        from src.agents.terminology.preprocess import TerminologyPreprocessor
        
        print(f"\n{'='*60}")
        print("æœ¯è¯­æ‰¹é‡é¢„å¤„ç†")
        print(f"{'='*60}")
        
        # ç¡®å®šæœ¯è¯­åº“è·¯å¾„
        term_db_path = Path(args.term_db) if Path(args.term_db).is_absolute() else Path(__file__).parent / args.term_db
        
        # åˆ›å»ºé¢„å¤„ç†å™¨
        preprocessor = TerminologyPreprocessor(
            src_lang='zh',
            tgt_lang='en',
            domain='law',
            db_path=str(term_db_path),
            max_concurrent=args.max_concurrent,
            batch_size=20
        )
        
        # æ‰§è¡Œé¢„å¤„ç†
        output_file = Path(args.output_dir) / f"preprocessed_terms_{int(time.time())}.json"
        stats = await preprocessor.preprocess_dataset(
            samples=samples,
            output_file=output_file,
            verbose=True
        )
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        print(f"\n{'='*60}")
        print("æœ¯è¯­é¢„å¤„ç†ç»Ÿè®¡")
        print(f"{'='*60}")
        print(f"æ€»æ ·æœ¬æ•°: {stats['total_samples']}")
        print(f"æå–æœ¯è¯­æ•°: {stats['total_extracted']}")
        print(f"åŽ»é‡åŽæœ¯è¯­æ•°: {stats['deduplicated']}")
        print(f"ä»Žæ•°æ®åº“èŽ·å–: {stats['from_database']}")
        print(f"æ–°ç¿»è¯‘æœ¯è¯­: {stats['from_llm']}")
        print(f"å¯¼å…¥æ•°æ®åº“: {stats['imported_to_db']}")
        print(f"ç»“æžœæ–‡ä»¶: {output_file}")
        print(f"{'='*60}\n")
        
        # å¦‚æžœä»…é¢„å¤„ç†ï¼Œåˆ™é€€å‡º
        if args.preprocess_only:
            print("âœ… æœ¯è¯­é¢„å¤„ç†å®Œæˆï¼")
            return 0
    
    # æ¶ˆèžå®žéªŒé…ç½®ï¼ˆæ¸è¿›å¼å››ç§å®žéªŒï¼‰
    ablation_configs = {
        'baseline': {
            'name': 'åŸºçº¿ï¼ˆçº¯LLMï¼‰',
            'hierarchical': False,
            'useTermBase': False,
            'useTM': False,
            'max_rounds': 1
        },
        'terminology': {
            'name': 'æœ¯è¯­æŽ§åˆ¶',
            'hierarchical': True,
            'useTermBase': True,
            'useTM': False,
            'max_rounds': 1  # åªè¿è¡Œæœ¯è¯­å±‚
        },
        'terminology_syntax': {
            'name': 'æœ¯è¯­+å¥æ³•æŽ§åˆ¶',
            'hierarchical': True,
            'useTermBase': True,
            'useTM': False,
            'max_rounds': 2  # è¿è¡Œæœ¯è¯­å±‚å’Œå¥æ³•å±‚
        },
        'full': {
            'name': 'å®Œæ•´ç³»ç»Ÿï¼ˆæœ¯è¯­+å¥æ³•+ç¯‡ç« ï¼‰',
            'hierarchical': True,
            'useTermBase': True,
            'useTM': True,  # é»˜è®¤å¯ç”¨TM
            'max_rounds': 3  # è¿è¡Œæ‰€æœ‰ä¸‰å±‚
        }
    }
    
    # è¿è¡Œå®žéªŒ
    runner = ExperimentRunner(args.output_dir, max_concurrent=args.max_concurrent)
    all_results = {}
    
    batch_mode = not args.no_batch
    
    for ablation_name in args.ablations:
        if ablation_name in ablation_configs:
            config = ablation_configs[ablation_name]
            
            # æ·»åŠ LLMé€‰æ‹©å™¨é…ç½®
            config['selection_layers'] = args.selection_layers
            config['num_candidates'] = args.num_candidates
            
            # æ·»åŠ é—¨æŽ§é…ç½®
            config['gating_layers'] = args.gating_layers
            config['term_gate_threshold'] = args.term_gate_threshold
            config['syntax_gate_threshold'] = args.syntax_gate_threshold
            config['discourse_gate_threshold'] = args.discourse_gate_threshold
            config['tm_gate_threshold'] = args.tm_gate_threshold
            
            # å¯¹äºŽfullå®žéªŒä¸”å¼€å¯save_intermediateæ—¶ï¼Œä¿å­˜ä¸­é—´ç»“æžœ
            save_intermediate = args.save_intermediate and ablation_name == 'full'
            results = await runner.run_ablation(
                samples, 
                ablation_name, 
                config, 
                verbose=args.verbose, 
                batch_mode=batch_mode,
                save_intermediate=save_intermediate,
                enable_quality_assessment=args.enable_quality_assessment
            )
            all_results[ablation_name] = results
            
            # å¦‚æžœæ˜¯fullå®žéªŒä¸”ä¿å­˜äº†ä¸­é—´ç»“æžœï¼Œè‡ªåŠ¨ç”Ÿæˆterminologyå’Œterminology_syntaxçš„ç»“æžœ
            if save_intermediate and ablation_name == 'full':
                print(f"\n{'='*60}")
                print("ä»Žfullå®žéªŒä¸­æå–ä¸­é—´å±‚ç»“æžœ...")
                print(f"{'='*60}")
                
                # è°ƒè¯•ï¼šæ£€æŸ¥æœ‰å¤šå°‘ç»“æžœåŒ…å«intermediateå­—æ®µ
                samples_with_intermediate = sum(1 for sample in results if 'intermediate' in sample)
                print(f"ðŸ“Š åŒ…å«intermediateå­—æ®µçš„æ ·æœ¬: {samples_with_intermediate}/{len(results)}")
                
                # æå–æœ¯è¯­å±‚ç»“æžœ
                terminology_results = []
                for sample in results:
                    if 'intermediate' in sample and 'round1_terminology' in sample['intermediate']:
                        terminology_results.append({
                            'sample_id': sample['sample_id'],
                            'source': sample['source'],
                            'target': sample['target'],
                            'prediction': sample['intermediate']['round1_terminology']['prediction'],
                            'success': True,
                            'metrics': {},
                            'metadata': sample.get('metadata', {}),
                            'terms_used': sample['intermediate']['round1_terminology'].get('terms_used', 0),
                            'confidence': sample['intermediate']['round1_terminology'].get('confidence', 0.0)
                        })
                    elif 'intermediate' in sample:
                        # è°ƒè¯•ï¼šæ‰“å°intermediateçš„keys
                        if not terminology_results and len(terminology_results) < 3:  # åªæ‰“å°å‰3ä¸ª
                            print(f"  âš ï¸  æ ·æœ¬ {sample['sample_id']} æœ‰intermediateä½†ç¼ºå°‘round1_terminology")
                            print(f"      intermediate keys: {list(sample['intermediate'].keys())}")
                
                if terminology_results:
                    all_results['terminology'] = terminology_results
                    print(f"âœ“ æå–äº† {len(terminology_results)} ä¸ªæœ¯è¯­å±‚ç»“æžœ")
                else:
                    print(f"âš ï¸  æœªèƒ½æå–æœ¯è¯­å±‚ç»“æžœ")
                
                # æå–æœ¯è¯­+å¥æ³•å±‚ç»“æžœ
                syntax_results = []
                for sample in results:
                    if 'intermediate' in sample and 'round2_syntax' in sample['intermediate']:
                        syntax_results.append({
                            'sample_id': sample['sample_id'],
                            'source': sample['source'],
                            'target': sample['target'],
                            'prediction': sample['intermediate']['round2_syntax']['prediction'],
                            'success': True,
                            'metrics': {},
                            'metadata': sample.get('metadata', {}),
                            'confidence': sample['intermediate']['round2_syntax'].get('confidence', 0.0)
                        })
                    elif 'intermediate' in sample:
                        # è°ƒè¯•ï¼šæ‰“å°intermediateçš„keys
                        if not syntax_results and len(syntax_results) < 3:  # åªæ‰“å°å‰3ä¸ª
                            print(f"  âš ï¸  æ ·æœ¬ {sample['sample_id']} æœ‰intermediateä½†ç¼ºå°‘round2_syntax")
                            print(f"      intermediate keys: {list(sample['intermediate'].keys())}")
                
                if syntax_results:
                    all_results['terminology_syntax'] = syntax_results
                    print(f"âœ“ æå–äº† {len(syntax_results)} ä¸ªæœ¯è¯­+å¥æ³•å±‚ç»“æžœ")
                else:
                    print(f"âš ï¸  æœªèƒ½æå–æœ¯è¯­+å¥æ³•å±‚ç»“æžœ")
                
                if terminology_results or syntax_results:
                    print(f"âœ“ ä»Ž1æ¬¡fullå®žéªŒè‡ªåŠ¨ç”Ÿæˆäº† {1 + bool(terminology_results) + bool(syntax_results)} ä¸ªæ¶ˆèžå®žéªŒç»“æžœï¼")
                else:
                    print(f"âŒ æœªèƒ½ä»Žfullå®žéªŒä¸­æå–ä¸­é—´å±‚ç»“æžœï¼Œå¯èƒ½traceæ•°æ®ä¸å®Œæ•´")
    
    # ä¿å­˜ç»“æžœ
    output_file = runner.save_results(all_results)
    
    print(f"\n{'='*60}")
    print("å®žéªŒå®Œæˆï¼")
    print(f"{'='*60}")
    print(f"ç»“æžœæ–‡ä»¶: {output_file}")
    print()
    
    return 0


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)

